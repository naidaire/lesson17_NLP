{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Natural Language Processing\n",
    " \n",
    "_Authors: Kiefer Katovich (SF), Joseph Nelson (DC)_\n",
    " \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Discuss the major tasks involved with natural language processing\n",
    "- Discuss, on a low level, the components of natural language processing\n",
    "- Identify why natural language processing is difficult\n",
    "- Demonstrate common text preprocessing techniques\n",
    "- Demonstrate text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To install textblob run:**\n",
    "\n",
    "1.\n",
    "`conda install -c https://conda.anaconda.org/sloria textblob` \n",
    "\n",
    "**or** \n",
    "\n",
    "`pip install textblob`  \n",
    "_&_  \n",
    "`python -m textblob.download_corpora lite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Guide\n",
    "\n",
    "- [Introduction to Natural Language Processing](#intro)\n",
    "- [Reading Yelp reviews with NLP](#yelp_rev)\n",
    "- [Text Classification](#text_class)\n",
    "- [Count Vectorization and Tokenization](#count_vec)\n",
    "\t- [Stopword Removal](#stopwords)\n",
    "\t- [Count Vector Options](#cvec_opt)\n",
    "- [Intro to TextBlob](#textblob)\n",
    "\t- [Stemming and Lemmatization](#stem)\n",
    "- [Term Frequency Inverse Document Frequency Vectorization](tfidf)\n",
    "\t- [Yelp Summary using TFIDF](#yelp_tfidf)\n",
    "- [Sentiment Analysis](#sentiment)\n",
    "- [BONUS: Adding Features to a Document Term Matrix](#add_feat)\n",
    "- [BONUS: More TextBlob Features](#more_textblob)\n",
    "- [BONUS: Intro to Naive Bayes & Text Classification](#bayes)\n",
    "- [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "*Adapted from [NLP Crash Course](http://files.meetup.com/7616132/DC-NLP-2013-09%20Charlie%20Greenbacker.pdf) by Charlie Greenbacker and [Introduction to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf) by Dan Jurafsky*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "\n",
    "- Using computers to process (analyze, understand, generate) natural human languages.\n",
    "- Most knowledge created by humans is unstructured text, and we need a way to make sense of it.\n",
    "- Build probabilistic model using data about a language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the higher level task areas?\n",
    "\n",
    "- **Information retrieval**: Find relevant results and similar results\n",
    "    - [Google](https://www.google.com/)\n",
    "- **Information extraction**: Structured information from unstructured documents\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation**: One language to another\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input**: Faster or easier typing\n",
    "    - [Phrase completion application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis**: Attitude of speaker\n",
    "    - [Hater News](http://haternews.herokuapp.com/)\n",
    "- **Automatic summarization**: Extractive or abstractive summarization\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural Language Generation**: Generate text from data\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation**: Speech-to-text, text-to-speech\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some of the lower level components?\n",
    "\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: a/an/the\n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\"\n",
    "- **Machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "Natural Language Processing requires an understanding of the **language** and the **world**. Several limiations faced with NLP involve:\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: text messages\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yelp_rev'></a>\n",
    "\n",
    "## Reading in the Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"corpus\" = collection of documents\n",
    "- \"corpora\" = plural form of corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read yelp.csv into a DataFrame\n",
    "path = r'./assets/dataset/yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "\n",
    "# create a new DataFrame that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# split the new DataFrame into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the head of the original data\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='text_class'></a>\n",
    "\n",
    "\n",
    "\n",
    "# Introduction: Text Classification\n",
    "Objective: Demonstrate how to classify text or documents using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text classification is the task of predicting which category or topic a text sample is from.**\n",
    "\n",
    "- We may want to identify whether an article is a sports or business story.  \n",
    "- Whether an article has positive or negative sentiment.\n",
    "- Possibly, how well is a recipe rated?\n",
    "\n",
    "**Typically, this is done by using the text as features and the label as the target output.**\n",
    "\n",
    "- This is referred to as bag-of-words classification.\n",
    "- To include text as features, we usually create a binary feature for each word, i.e. does this piece of text contain that word?\n",
    "- To create binary text features, we first create a vocabulary to account for all possible words in our universe.  \n",
    "\n",
    "**As we do this, we need to consider several things.**\n",
    "\n",
    "- Does order of words matter?\n",
    "- Does punctuation matter?\n",
    "- Does upper or lower case matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Text processing in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='count_vec'></a>\n",
    "\n",
    "\n",
    "\n",
    "### Tokenization with CountVectorizer\n",
    "\n",
    "- **What:** Separates text into units such as sentences or words.\n",
    "- **Why:** Gives structure to previously unstructured text.\n",
    "- **Notes:** Relatively easy with English language text, not easy with some languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use CountVectorizer to create document-term matrices from X_train and X_test\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16825)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows are documents, columns are terms (aka \"tokens\" or \"features\", individual words in this situation)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'yyyyy', u'z11', u'za', u'zabba', u'zach', u'zam', u'zanella', u'zankou', u'zappos', u'zatsiki', u'zen', u'zero', u'zest', u'zexperience', u'zha', u'zhou', u'zia', u'zihuatenejo', u'zilch', u'zin', u'zinburger', u'zinburgergeist', u'zinc', u'zinfandel', u'zing', u'zip', u'zipcar', u'zipper', u'zippers', u'zipps', u'ziti', u'zoe', u'zombi', u'zombies', u'zone', u'zones', u'zoning', u'zoo', u'zoyo', u'zucca', u'zucchini', u'zuchinni', u'zumba', u'zupa', u'zuzu', u'zwiebel', u'zzed', u'\\xe9clairs', u'\\xe9cole', u'\\xe9m']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'zoning',\n",
       " u'zoo',\n",
       " u'zucchini',\n",
       " u'zuchinni',\n",
       " u'zupa',\n",
       " u'zwiebel',\n",
       " u'zzed',\n",
       " u'\\xc9cole',\n",
       " u'\\xe9clairs',\n",
       " u'\\xe9m']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't convert to lowercase\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape\n",
    "vect.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 169847)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start to see how n-gram length can make out data exponentially larger.  By going from a vectorization of all words that appear in the corpus to a vectorization of all words _as-well-as_ combinations of 2 words that appear together in the corpus, we have increased the number of features in our dataset ten-fold.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'zone out', u'zone when', u'zones', u'zones dolls', u'zoning', u'zoning issues', u'zoo', u'zoo and', u'zoo is', u'zoo not', u'zoo the', u'zoo ve', u'zoyo', u'zoyo for', u'zucca', u'zucca appetizer', u'zucchini', u'zucchini and', u'zucchini bread', u'zucchini broccoli', u'zucchini carrots', u'zucchini fries', u'zucchini pieces', u'zucchini strips', u'zucchini veal', u'zucchini very', u'zucchini with', u'zuchinni', u'zuchinni again', u'zuchinni the', u'zumba', u'zumba class', u'zumba or', u'zumba yogalates', u'zupa', u'zupa flavors', u'zuzu', u'zuzu in', u'zuzu is', u'zuzu the', u'zwiebel', u'zwiebel kr\\xe4uter', u'zzed', u'zzed in', u'\\xe9clairs', u'\\xe9clairs napoleons', u'\\xe9cole', u'\\xe9cole len\\xf4tre', u'\\xe9m', u'\\xe9m all']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting the star rating:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918786692759\n"
     ]
    }
   ],
   "source": [
    "# use default options for CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# create document-term matrices\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "# use Naive Bayes to predict the star rating\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    838\n",
       "1    184\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent 5 Stars: 0.819960861057\n",
      "Precent 1 Stars: 0.180039138943\n"
     ]
    }
   ],
   "source": [
    "# calculate null accuracy\n",
    "y_test_binary = np.where(y_test==5, 1, 0) # five starts become 1, one starts become 0\n",
    "print 'Percent 5 Stars:', y_test_binary.mean()\n",
    "print 'Precent 1 Stars:', 1 - y_test_binary.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Baseline is around 82._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer and calculates the accuracy\n",
    "def tokenize_test(vect):\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    print('Features: ', X_train_dtm.shape[1])\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 10000)\n",
      "('Accuracy: ', 0.90704500978473579)\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(1, 3), min_df=2, max_features=10000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>\n",
    "\n",
    "### Stopword Removal\n",
    "\n",
    "- **What:** Remove common words that will likely appear in any text\n",
    "- **Why:** They don't tell you much about your text\n",
    "\n",
    "**What are stop words?**\n",
    "Stops words are some of the most common words in a language that do not tend to carry meaning, but in most cases are used so that a sentence makes sence gramitically, such as prepositions and determiners, i.e., 'to', 'the', 'and'.\n",
    "\n",
    "Example: \n",
    "\n",
    "> 1. \"The dog jumped over the fence\"  \n",
    "> 2. \"Dog jumped over fence\"\n",
    "\n",
    "The fact that there is a fence and a dog jumped over it can be derived with or without stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=10000, min_df=2,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "- If 'english', a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 16528)\n",
      "('Accuracy: ', 0.91585127201565553)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'analyzer': u'word',\n",
       " 'binary': False,\n",
       " 'decode_error': u'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': u'utf-8',\n",
       " 'input': u'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': 'english',\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove English stop words\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "tokenize_test(vect)\n",
    "vect.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset(['all', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'fifty', 'four', 'not', 'own', 'through', 'yourselves', 'go', 'where', 'mill', 'only', 'find', 'before', 'one', 'whose', 'system', 'how', 'somewhere', 'with', 'thick', 'show', 'had', 'enough', 'should', 'to', 'must', 'whom', 'seeming', 'under', 'ours', 'has', 'might', 'thereafter', 'latterly', 'do', 'them', 'his', 'around', 'than', 'get', 'very', 'de', 'none', 'cannot', 'every', 'whether', 'they', 'front', 'during', 'thus', 'now', 'him', 'nor', 'name', 'several', 'hereafter', 'always', 'who', 'cry', 'whither', 'this', 'someone', 'either', 'each', 'become', 'thereupon', 'sometime', 'side', 'two', 'therein', 'twelve', 'because', 'often', 'ten', 'our', 'eg', 'some', 'back', 'up', 'namely', 'towards', 'are', 'further', 'beyond', 'ourselves', 'yet', 'out', 'even', 'will', 'what', 'still', 'for', 'bottom', 'mine', 'since', 'please', 'forty', 'per', 'its', 'everything', 'behind', 'un', 'above', 'between', 'it', 'neither', 'seemed', 'ever', 'across', 'she', 'somehow', 'be', 'we', 'full', 'never', 'sixty', 'however', 'here', 'otherwise', 'were', 'whereupon', 'nowhere', 'although', 'found', 'alone', 're', 'along', 'fifteen', 'by', 'both', 'about', 'last', 'would', 'anything', 'via', 'many', 'could', 'thence', 'put', 'against', 'keep', 'etc', 'amount', 'became', 'ltd', 'hence', 'onto', 'or', 'con', 'among', 'already', 'co', 'afterwards', 'formerly', 'within', 'seems', 'into', 'others', 'while', 'whatever', 'except', 'down', 'hers', 'everyone', 'done', 'least', 'another', 'whoever', 'moreover', 'couldnt', 'throughout', 'anyhow', 'yourself', 'three', 'from', 'her', 'few', 'together', 'top', 'there', 'due', 'been', 'next', 'anyone', 'eleven', 'much', 'call', 'therefore', 'interest', 'then', 'thru', 'themselves', 'hundred', 'was', 'sincere', 'empty', 'more', 'himself', 'elsewhere', 'mostly', 'on', 'fire', 'am', 'becoming', 'hereby', 'amongst', 'else', 'part', 'everywhere', 'too', 'herself', 'former', 'those', 'he', 'me', 'myself', 'made', 'twenty', 'these', 'bill', 'cant', 'us', 'until', 'besides', 'nevertheless', 'below', 'anywhere', 'nine', 'can', 'of', 'your', 'toward', 'my', 'something', 'and', 'whereafter', 'whenever', 'give', 'almost', 'wherever', 'is', 'describe', 'beforehand', 'herein', 'an', 'as', 'itself', 'at', 'have', 'in', 'seem', 'whence', 'ie', 'any', 'fill', 'again', 'hasnt', 'inc', 'thereby', 'thin', 'no', 'perhaps', 'latter', 'meanwhile', 'when', 'detail', 'same', 'wherein', 'beside', 'also', 'that', 'other', 'take', 'which', 'becomes', 'you', 'if', 'nobody', 'see', 'though', 'may', 'after', 'upon', 'most', 'hereupon', 'eight', 'but', 'serious', 'nothing', 'such', 'why', 'a', 'off', 'whereby', 'third', 'i', 'whole', 'noone', 'sometimes', 'well', 'amoungst', 'yours', 'their', 'rather', 'without', 'so', 'five', 'the', 'first', 'whereas', 'once'])\n"
     ]
    }
   ],
   "source": [
    "# set of stop words\n",
    "print(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cvec_opt'></a>\n",
    "### Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_features:** int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.  This allows us to keep more common n-grams and remove ones that may appear once which can lead to said features being highly associated with a class and cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 100)\n",
      "('Accuracy: ', 0.86986301369863017)\n"
     ]
    }
   ],
   "source": [
    "# remove English stop words and only keep 100 features\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'amazing', u'area', u'atmosphere', u'awesome', u'bad', u'bar', u'best', u'better', u'big', u'came', u'cheese', u'chicken', u'clean', u'coffee', u'come', u'day', u'definitely', u'delicious', u'did', u'didn', u'dinner', u'don', u'eat', u'excellent', u'experience', u'favorite', u'feel', u'food', u'free', u'fresh', u'friendly', u'friends', u'going', u'good', u'got', u'great', u'happy', u'home', u'hot', u'hour', u'just', u'know', u'like', u'little', u'll', u'location', u'long', u'looking', u'lot', u'love', u'lunch', u'make', u'meal', u'menu', u'minutes', u'need', u'new', u'nice', u'night', u'order', u'ordered', u'people', u'perfect', u'phoenix', u'pizza', u'place', u'pretty', u'prices', u'really', u'recommend', u'restaurant', u'right', u'said', u'salad', u'sandwich', u'sauce', u'say', u'service', u'staff', u'store', u'sure', u'table', u'thing', u'things', u'think', u'time', u'times', u'took', u'town', u'tried', u'try', u've', u'wait', u'want', u'way', u'went', u'wine', u'work', u'worth', u'years']\n"
     ]
    }
   ],
   "source": [
    "# all 100 features\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with all other modeals more features does not mean a better model and we must tune to remove features whose predictive capability is none or very low.\n",
    "\n",
    "There is roughly a 1.6% increase in accuracy when we double out n-gram size and increase our max features by one-thousand fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 100000)\n",
      "('Accuracy: ', 0.88551859099804309)\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and limit the number of features\n",
    "vect = CountVectorizer(ngram_range=(1, 2), max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 43957)\n",
      "('Accuracy: ', 0.93248532289628183)\n"
     ]
    }
   ],
   "source": [
    "# include 1-grams and 2-grams, and only include terms that appear at least 2 times\n",
    "vect = CountVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='textblob'></a>\n",
    "## Introduction to TextBlob\n",
    "\n",
    "TextBlob: \"Simplified Text Processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "# print the first review\n",
    "print(yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save it as a TextBlob object\n",
    "review = TextBlob(yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'The', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'Our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', 'was', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', 'looks', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', 'was', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the words\n",
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"My wife took me here on my birthday for breakfast and it was excellent.\"),\n",
       " Sentence(\"The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.\"),\n",
       " Sentence(\"Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.\"),\n",
       " Sentence(\"It looked like the place fills up pretty quickly so the earlier you get here the better.\"),\n",
       " Sentence(\"Do yourself a favor and get their Bloody Mary.\"),\n",
       " Sentence(\"It was phenomenal and simply the best I've ever had.\"),\n",
       " Sentence(\"I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.\"),\n",
       " Sentence(\"It was amazing.\"),\n",
       " Sentence(\"While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.\"),\n",
       " Sentence(\"It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.\"),\n",
       " Sentence(\"It was the best \"toast\" I've ever had.\"),\n",
       " Sentence(\"Anyway, I can't wait to go back!\")]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list the sentences\n",
    "review.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"my wife took me here on my birthday for breakfast and it was excellent.  the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  our waitress was excellent and our food arrived quickly on the semi-busy saturday morning.  it looked like the place fills up pretty quickly so the earlier you get here the better.\n",
       "\n",
       "do yourself a favor and get their bloody mary.  it was phenomenal and simply the best i've ever had.  i'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  it was amazing.\n",
       "\n",
       "while everything on the menu looks excellent, i had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  it came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  it was the best \"toast\" i've ever had.\n",
       "\n",
       "anyway, i can't wait to go back!\")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some string methods are available\n",
    "review.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stem'></a>\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "**Stemming** is a crude process of removing common endings from sentences, such as ‘s’, ‘es’, ‘ly’, ‘ing’, and ‘ed’.\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'my', u'wife', u'took', u'me', u'here', u'on', u'my', u'birthday', u'for', u'breakfast', u'and', u'it', u'was', u'excel', u'the', u'weather', u'was', u'perfect', u'which', u'made', u'sit', u'outsid', u'overlook', u'their', u'ground', u'an', u'absolut', u'pleasur', u'our', u'waitress', u'was', u'excel', u'and', u'our', u'food', u'arriv', u'quick', u'on', u'the', u'semi-busi', u'saturday', u'morn', u'it', u'look', u'like', u'the', u'place', u'fill', u'up', u'pretti', u'quick', u'so', u'the', u'earlier', u'you', u'get', u'here', u'the', u'better', u'do', u'yourself', u'a', u'favor', u'and', u'get', u'their', u'bloodi', u'mari', u'it', u'was', u'phenomen', u'and', u'simpli', u'the', u'best', u'i', u've', u'ever', u'had', u'i', u\"'m\", u'pretti', u'sure', u'they', u'onli', u'use', u'ingredi', u'from', u'their', u'garden', u'and', u'blend', u'them', u'fresh', u'when', u'you', u'order', u'it', u'it', u'was', u'amaz', u'while', u'everyth', u'on', u'the', u'menu', u'look', u'excel', u'i', u'had', u'the', u'white', u'truffl', u'scrambl', u'egg', u'veget', u'skillet', u'and', u'it', u'was', u'tasti', u'and', u'delici', u'it', u'came', u'with', u'2', u'piec', u'of', u'their', u'griddl', u'bread', u'with', u'was', u'amaz', u'and', u'it', u'absolut', u'made', u'the', u'meal', u'complet', u'it', u'was', u'the', u'best', u'toast', u'i', u've', u'ever', u'had', u'anyway', u'i', u'ca', u\"n't\", u'wait', u'to', u'go', u'back']\n"
     ]
    }
   ],
   "source": [
    "# initialize stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# stem each word\n",
    "print([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are \"excellent\" stemmed to \"excel\" and \"amazing\" stemmed to \"amaz\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization** is a more refined process that uses specific language and grammar rules to derive the root of a word.  \n",
    "\n",
    "This is useful for words that do not share an obvious root such as ‘better’ and ‘best’.\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', u'wa', 'excellent', 'The', 'weather', u'wa', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', u'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', u'wa', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', 'looked', 'like', 'the', 'place', u'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', u'wa', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', 'had', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', u'ingredient', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', u'wa', 'amazing', 'While', 'EVERYTHING', 'on', 'the', 'menu', u'look', 'excellent', 'I', 'had', 'the', 'white', 'truffle', 'scrambled', u'egg', 'vegetable', 'skillet', 'and', 'it', u'wa', 'tasty', 'and', 'delicious', 'It', 'came', 'with', '2', u'piece', 'of', 'their', 'griddled', 'bread', 'with', u'wa', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'It', u'wa', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', 'had', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a noun\n",
    "print([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are \"filled\" lammatized to \"fill\" and \"was\" stemmed to \"wa\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'wife', u'take', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', u'be', 'excellent', 'The', 'weather', u'be', 'perfect', 'which', u'make', u'sit', 'outside', u'overlook', 'their', u'ground', 'an', 'absolute', 'pleasure', 'Our', 'waitress', u'be', 'excellent', 'and', 'our', 'food', u'arrive', 'quickly', 'on', 'the', 'semi-busy', 'Saturday', 'morning', 'It', u'look', 'like', 'the', 'place', u'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'Do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'Bloody', 'Mary', 'It', u'be', 'phenomenal', 'and', 'simply', 'the', 'best', 'I', \"'ve\", 'ever', u'have', 'I', \"'m\", 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'It', u'be', u'amaze', 'While', 'EVERYTHING', 'on', 'the', 'menu', u'look', 'excellent', 'I', u'have', 'the', 'white', 'truffle', u'scramble', u'egg', 'vegetable', 'skillet', 'and', 'it', u'be', 'tasty', 'and', 'delicious', 'It', u'come', 'with', '2', u'piece', 'of', 'their', u'griddle', 'bread', 'with', u'be', u'amaze', 'and', 'it', 'absolutely', u'make', 'the', 'meal', 'complete', 'It', u'be', 'the', 'best', 'toast', 'I', \"'ve\", 'ever', u'have', 'Anyway', 'I', 'ca', \"n't\", 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "# assume every word is a verb\n",
    "print([word.lemmatize(pos='v') for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are \"was\" lammantized to \"be\" and \"arrived\" lammantized to \"arrive\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Lemmatization and Stemming examples**\n",
    "\n",
    "|Lemmatization|Stemming|\n",
    "|-------------|---------|\n",
    "|shouted → shout|badly → bad|\n",
    "|best → good|computing → comput|\n",
    "|better → good|computed → comput|\n",
    "|good → good|wipes → wip|\n",
    "|wiping → wipe|wiped → wip|\n",
    "|hidden → hide|wiping → wip|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTIVITY:  KNOWLEDGE CHECK\n",
    "- What other words or phrases might cause problems with stemming? Why?\n",
    "- What other words or phrases might cause problems with lemmatization? Why?\n",
    "\n",
    "----\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    text = unicode(text, 'utf-8').lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Features: ', 16452)\n",
      "('Accuracy: ', 0.92074363992172215)\n"
     ]
    }
   ],
   "source": [
    "# use split_into_lemmas as the feature extraction function (WARNING: SLOW!)\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas, decode_error='replace')\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'yuyuyummy', u'yuzu', u'z', u'z-grill', u'z11', u'zach', u'zam', u'zanella', u'zankou', u'zappos', u'zatsiki', u'zen', u'zen-like', u'zero', u'zero-star', u'zest', u'zexperience', u'zha', u'zhou', u'zia', u'zilch', u'zin', u'zinburger', u'zinburgergeist', u'zinc', u'zinfandel', u'zing', u'zip', u'zipcar', u'zipper', u'zipps', u'ziti', u'zoe', u'zombi', u'zombie', u'zone', u'zoning', u'zoo', u'zoyo', u'zucca', u'zucchini', u'zuchinni', u'zumba', u'zupa', u'zuzu', u'zwiebel-kr\\xe4uter', u'zzed', u'\\xe9clairs', u'\\xe9cole', u'\\xe9m']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tfidf'></a>\n",
    "## Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents)\n",
    "- **Notes:** Used for search engine scoring, text summarization, document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1, 6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency-Inverse Document Frequency (simple version)\n",
    "tf/df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More details:** [TF-IDF is about what matters](http://planspace.org/20150524-tfidf_is_about_what_matters/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yelp_tfidf'></a>\n",
    "## Using TF-IDF to Summarize a Yelp Review\n",
    "\n",
    "Reddit's autotldr uses the [SMMRY](http://smmry.com/about) algorithm, which is based on TF-IDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28880)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a document-term matrix using TF-IDF\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "# fit transform yelp data\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    \n",
    "    # choose a random review that is at least 300 characters\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = yelp.text[review_id]\n",
    "        #review_text = unicode(yelp.text[review_id], 'utf-8')\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # create a dictionary of words and their TF-IDF scores\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # print words with the top 5 TF-IDF scores\n",
    "    print('TOP SCORING WORDS:')\n",
    "    top_scores = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print(word)\n",
    "    \n",
    "    # print 5 random words\n",
    "    print('\\n' + 'RANDOM WORDS:')\n",
    "    random_words = np.random.choice(list(word_scores.keys()), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print(word)\n",
    "    \n",
    "    # print the review\n",
    "    print('\\n' + review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP SCORING WORDS:\n",
      "yogurtland\n",
      "ygl\n",
      "spoons\n",
      "favorite\n",
      "flavors\n",
      "\n",
      "RANDOM WORDS:\n",
      "hubbie\n",
      "thinking\n",
      "partner\n",
      "cone\n",
      "yogurt\n",
      "\n",
      "\"Yogurtland\" as I say it with a high-pitched tone in the end..\n",
      "\n",
      "This is my ultimate favorite dessert!! I would give 10 stars if Yelp allows it! All my friends know that I am attached to this place! We started this craziness together with my better half when we tried it a few years back after a basketball game...and all I said after first walking in was, \"This is fun!!\" and we got hooked eversince!! That I collect their spoons, only used by me and the hubbie, and I now have 5 YGL  cups full of spoons!! I am the ultimate Yogurtland addict!! heehee We go as often as 5 times a week that we already know the staff in this location. And we go that often that we even tried to abstain from it for Lent just to see if we can resist it!! haha Most of our friends also keep on asking to try other fro yo places to see if something will beat my YGL addiction..and so far nothing has won over it yet..=D\n",
      "So, my favorite flavors are Taro, Pistachio and Toasted Coconut and the of course the seasonal flavors, Pumpkin Pie, Gingerbread..Oh I wanna go now just thinking about it! My favorite toppings are the yogurt chips, sugar cone, sunflower seeds, shredded coconut and graham!!  =) Perfect blend of flavors! \n",
      "\n",
      "So if you haven't tried it yet, grab your favorite partner and enjoy some \"Yogurtland\"...=)\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sentiment'></a>\n",
    "## Sentiment Analysis\n",
    "\n",
    "Understanding how positive or negative a review is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\n",
      "\n",
      "Do yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I've ever had.  I'm pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\n",
      "\n",
      "While EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I've ever had.\n",
      "\n",
      "Anyway, I can't wait to go back!\n"
     ]
    }
   ],
   "source": [
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40246913580246907"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# polarity ranges from -1 (most negative) to 1 (most positive)\n",
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  length  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0     889  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understanding the apply method\n",
    "yelp['length'] = yelp.text.apply(len)\n",
    "yelp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns the polarity\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text.decode('utf-8')).sentiment.polarity\n",
    "    #return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new DataFrame column for sentiment (WARNING: SLOW!)\n",
    "yelp['sentiment'] = yelp.text.apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1199d4090>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEcCAYAAAAGD4lRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCEFKGQsdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1187a1e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# box plot of sentiment grouped by stars\n",
    "yelp.boxplot(column='sentiment', by='stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254    Our server Gary was awesome. Food was amazing....\n",
       "347    3 syllables for this place. \\nA-MAZ-ING!\\n\\nTh...\n",
       "420                                    LOVE the food!!!!\n",
       "459    Love it!!! Wish we still lived in Arizona as C...\n",
       "679                                     Excellent burger\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews with most positive sentiment\n",
    "yelp[yelp.sentiment == 1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773     This was absolutely horrible. I got the suprem...\n",
       "1517                  Nasty workers and over priced trash\n",
       "3266    Absolutely awful... these guys have NO idea wh...\n",
       "4766                                       Very bad food!\n",
       "5812        I wouldn't send my worst enemy to this place.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reviews with most negative sentiment\n",
    "yelp[yelp.sentiment == -1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# widen the column display\n",
    "pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>106JT5p8e8Chtd0CZpcARw</td>\n",
       "      <td>2009-08-06</td>\n",
       "      <td>KowGVoP_gygzdSu6Mt3zKQ</td>\n",
       "      <td>5</td>\n",
       "      <td>RIP AZ Coffee Connection.  :(  I stopped by two days ago unaware that they had closed.  I am severely bummed.  This place is irreplaceable!  Damn you, Starbucks and McDonalds!</td>\n",
       "      <td>review</td>\n",
       "      <td>jKeaOrPyJ-dI9SNeVqrbww</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>-0.302083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                business_id        date               review_id  stars  \\\n",
       "390  106JT5p8e8Chtd0CZpcARw  2009-08-06  KowGVoP_gygzdSu6Mt3zKQ      5   \n",
       "\n",
       "                                                                                                                                                                                text  \\\n",
       "390  RIP AZ Coffee Connection.  :(  I stopped by two days ago unaware that they had closed.  I am severely bummed.  This place is irreplaceable!  Damn you, Starbucks and McDonalds!   \n",
       "\n",
       "       type                 user_id  cool  useful  funny  length  sentiment  \n",
       "390  review  jKeaOrPyJ-dI9SNeVqrbww     1       0      0     175  -0.302083  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative sentiment in a 5-star review\n",
    "yelp[(yelp.stars == 5) & (yelp.sentiment < -0.3)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>length</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>53YGfwmbW73JhFiemNeyzQ</td>\n",
       "      <td>2012-06-22</td>\n",
       "      <td>Gi-4O3EhE175vujbFGDIew</td>\n",
       "      <td>1</td>\n",
       "      <td>If you like the stuck up Scottsdale vibe this is a good place for you. The food isn't impressive. Nice outdoor seating.</td>\n",
       "      <td>review</td>\n",
       "      <td>Hqgx3IdJAAaoQjvrUnbNvw</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>119</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars  \\\n",
       "1781  53YGfwmbW73JhFiemNeyzQ  2012-06-22  Gi-4O3EhE175vujbFGDIew      1   \n",
       "\n",
       "                                                                                                                         text  \\\n",
       "1781  If you like the stuck up Scottsdale vibe this is a good place for you. The food isn't impressive. Nice outdoor seating.   \n",
       "\n",
       "        type                 user_id  cool  useful  funny  length  sentiment  \n",
       "1781  review  Hqgx3IdJAAaoQjvrUnbNvw     0       1      2     119   0.766667  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# positive sentiment in a 1-star review\n",
    "yelp[(yelp.stars == 1) & (yelp.sentiment > 0.5)].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset the column display width\n",
    "pd.reset_option('max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='add_feat'></a>\n",
    "## Bonus: Adding Features to a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a DataFrame that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
    "X = yelp_best_worst[feature_cols]\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064, 16825)\n",
      "(1022, 16825)\n"
     ]
    }
   ],
   "source": [
    "# use CountVectorizer with text column only\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of other four feature columns\n",
    "X_train.drop('text', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cast other feature columns to float and convert to a sparse matrix\n",
    "extra = sp.sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\n",
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16829)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine sparse matrices\n",
    "X_train_dtm_extra = sp.sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 16829)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat for testing set\n",
    "extra = sp.sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\n",
    "X_test_dtm_extra = sp.sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917808219178\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with text column only\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.922700587084\n"
     ]
    }
   ],
   "source": [
    "# use logistic regression with all features\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm_extra)\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='more_textblob'></a>\n",
    "## Bonus: Fun TextBlob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"15 minutes late\")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spelling correction\n",
    "TextBlob('15 minuets late').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('part', 0.9929478138222849), (u'parrot', 0.007052186177715092)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spellcheck\n",
    "Word('parot').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'tip laterally',\n",
       " u'enclose with a bank',\n",
       " u'do business with a bank or keep an account at a bank',\n",
       " u'act as the banker in a game or in gambling',\n",
       " u'be in the banking business',\n",
       " u'put into a bank account',\n",
       " u'cover with ashes so to control the rate of burning',\n",
       " u'have confidence or faith in']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definitions\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'es'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# language identification\n",
    "TextBlob('Hola amigos').detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bayes\"></a>\n",
    "\n",
    "## BONUS: Intro to Naive Bayes & Text Classification\n",
    "\n",
    "Earlier we experimented with Text Classification using a Naive Bayes Model, but what are Naive Bayes Classifiers? \n",
    "\n",
    "**What is Bayes?**  \n",
    "Bayes or Bayes Theorum is a different way to assess probability where prior information is considered in order to more accurately assess the situation.  \n",
    "\n",
    "Example: You are playing roulette.\n",
    "\n",
    "As you approach the table you see that the last number the ball landed on was Red-3.  With a frequntist mindset you know that the ball is just as likely to land on Red-3 again given that every slot on the wheel has an equal opportunity of 1 in 37.\n",
    "\n",
    "However, it is against your intuition to bet on Red on the next roll because you think that because it was red this time it is more likely to be black next time.  You don't know why, but in the back of your mind you believe that the ball is more likely to land on Black given it landed on Red previously than it is to land on Red twice in a row. \n",
    "\n",
    "This is what Bayes is all about, adjusting probabilities as more data is gathered!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the equation for Bayes.  \n",
    "\n",
    "$$P(A \\ | \\ B) = \\frac {P(B \\ | \\ A) \\times P(A)} {P(B)}$$\n",
    "\n",
    "- **$P(A \\ | \\ B)$** : Probability of `Event A` occuring given `Event B` has occured.\n",
    "- **$P(B \\ | \\ A)$** : Probability of `Event B` occuring given `Event A` has occured.\n",
    "- **$P(A)$** : Probability of `Event A` occuring.\n",
    "- **$P(B)$** : Probability of `Event B` occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Applying Naive Bayes classification to spam filtering\n",
    "\n",
    "Let's pretend we have an email with three words: \"Send money now.\" We'll use Naive Bayes to classify it as **ham or spam.**\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) = \\frac {P(\\text{send money now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "By assuming that the features (the words) are **conditionally independent**, we can simplify the likelihood function:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {P(\\text{send} \\ | \\ spam) \\times P(\\text{money} \\ | \\ spam) \\times P(\\text{now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "We can calculate all of the values in the numerator by examining a corpus of **spam email**:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {0.2 \\times 0.1 \\times 0.1 \\times 0.9} {P(\\text{send money now})} = \\frac {0.0018} {P(\\text{send money now})}$$\n",
    "\n",
    "We would repeat this process with a corpus of **ham email**:\n",
    "\n",
    "$$P(ham \\ | \\ \\text{send money now}) \\approx \\frac {0.05 \\times 0.01 \\times 0.1 \\times 0.1} {P(\\text{send money now})} = \\frac {0.000005} {P(\\text{send money now})}$$\n",
    "\n",
    "All we care about is whether spam or ham has the **higher probability**, and so we predict that the email is **spam**.\n",
    "\n",
    "\n",
    "### Key takeaways\n",
    "\n",
    "- The **\"naive\" assumption** of Naive Bayes (that the features are conditionally independent) is critical to making these calculations simple.\n",
    "- The **normalization constant** (the denominator) can be ignored since it's the same for all classes.\n",
    "- The **prior probability** is much less relevant once you have a lot of features.\n",
    "\n",
    "### Comparing Naive Bayes with other models\n",
    "\n",
    "Advantages of Naive Bayes:\n",
    "\n",
    "- Model training and prediction are very fast\n",
    "- Somewhat interpretable\n",
    "- No tuning is required\n",
    "- Features don't need scaling\n",
    "- Insensitive to irrelevant features (with enough observations)\n",
    "- Performs better than logistic regression when the training set is very small\n",
    "\n",
    "Disadvantages of Naive Bayes:\n",
    "\n",
    "- Predicted probabilities are not well-calibrated\n",
    "- Correlated features can be problematic (due to the independence assumption)\n",
    "- Can't handle negative features (with Multinomial Naive Bayes)\n",
    "- Has a higher \"asymptotic error\" than logistic regression\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## Conclusion\n",
    "\n",
    "- NLP is a gigantic field\n",
    "- Understanding the basics broadens the types of data you can work with\n",
    "- Simple techniques go a long way\n",
    "- Use scikit-learn for NLP whenever possible\n",
    "\n",
    "\n",
    "While we used SKLearn and TextBlob today, another popular python NLP library is [Spacy](https://spacy.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
