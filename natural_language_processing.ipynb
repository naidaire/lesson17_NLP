{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    " \n",
    "# Natural Language Processing\n",
    " \n",
    "_Authors: Kiefer Katovich (San Francisco), Joseph Nelson (Washington, D.C.)_\n",
    " \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "By the end of the lesson, you will be able to..\n",
    "\n",
    "- Explain some of the challenges that make natural language processing difficult.\n",
    "- Perform common text preprocessing techniques.\n",
    "- Perform text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='textblob_install'></a>\n",
    "\n",
    "### Install TextBlob\n",
    "\n",
    "The TextBlob Python library provides a simplified interface for exploring common NLP tasks including part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "\n",
    "To proceed with the lesson, first install TextBlob, as explained below. We tend to prefer Anaconda-based installations, since they tend to be tested with our other Anaconda packages.\n",
    "\n",
    "**To install textblob run:**\n",
    "\n",
    "> `conda install -c conda-forge textblob`\n",
    "\n",
    "**If that command doesn't work, try these:**\n",
    "\n",
    "> `pip install textblob`\n",
    "\n",
    "> `python -m textblob.download_corpora lite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "*Adapted from [NLP Crash Course](http://files.meetup.com/7616132/DC-NLP-2013-09%20Charlie%20Greenbacker.pdf) by Charlie Greenbacker and [Introduction to NLP](http://spark-public.s3.amazonaws.com/nlp/slides/intro.pdf) by Dan Jurafsky*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Is Natural Language Processing (NLP)?\n",
    "\n",
    "Using computers to process (analyze, understand, generate) natural human languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use NLP?\n",
    "\n",
    "An enormous amount of information is stored as text. Computers can process this information much faster than humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Some of the Higher-Level Task Areas?\n",
    "\n",
    "- **Chatbots:** Understand natural language from the user and return intelligent responses.\n",
    "    - [Api.ai](https://api.ai/)\n",
    "- **Information retrieval:** Find relevant results and similar results.\n",
    "    - [Google](https://www.google.com/)    \n",
    "- **Information extraction:** Structured information from unstructured documents.\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation:** One language to another.\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification:** Preserve the meaning of text, but simplify the grammar and vocabulary.\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input:** Faster or easier typing.\n",
    "    - [Phrase completion application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis:** Attitude of speaker.\n",
    "    - [Hater News](https://medium.com/@KevinMcAlear/building-hater-news-62062c58325c)\n",
    "- **Automatic summarization:** Extractive or abstractive summarization.\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural language generation:** Generate text from data.\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation:** Speech-to-text, text-to-speech.\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering:** Determine the intent of the question, match query with knowledge base, evaluate hypotheses.\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Some of the Lower-Level Components?\n",
    "\n",
    "- **Tokenization:** Breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stop-word removal:** a/an/the\n",
    "- **Stemming and lemmatization:** root word\n",
    "- **TF-IDF:** word importance\n",
    "- **Part-of-speech tagging:** noun/verb/adjective\n",
    "- **Named entity recognition:** person/organization/location\n",
    "- **Spelling correction:** \"New Yrok City\"\n",
    "- **Word sense disambiguation:** \"buy a mouse\"\n",
    "- **Segmentation:** \"New York City subway\"\n",
    "- **Language detection:** \"translate this page\"\n",
    "- **Vectorizing:** Turning documents into vectors of numbers for use in machine learning\n",
    "- **Machine learning:** specialized models that work well with text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is NLP hard?\n",
    "\n",
    "Natural language processing requires an understanding of the language and the world. Several limitations of NLP are:\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals Are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English:** text messages\n",
    "- **Idioms:** \"throw in the towel\"\n",
    "- **Newly coined words:** \"retweet\"\n",
    "- **Tricky entity names:** \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge:** \"Mary and Sue are sisters\", \"Mary and Sue are mothers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP terms\n",
    "\n",
    "- **corpus** (plural **corpora**): a collection of documents (derived from the Latin word for \"body\")\n",
    "- **document**: any item in a corpus (e.g. email, book chapter, tweet, article, or text message)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yelp_rev'></a>\n",
    "\n",
    "## Reading in the Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read yelp.csv into a DataFrame.\n",
    "path = Path('.', 'data', 'yelp.csv')\n",
    "yelp = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame that only contains the 5-star and 1-star reviews.\n",
    "\n",
    "yelp_best_worst = yelp.loc[(yelp.loc[:,'stars'] ==5) | (yelp.loc[:, 'stars'] == 1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>zp713qNhx8d9KCJJnrw1xA</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>riFQ3vxNpP4rWLk_CSri2A</td>\n",
       "      <td>5</td>\n",
       "      <td>Drop what you're doing and drive here. After I...</td>\n",
       "      <td>review</td>\n",
       "      <td>wFweIWhv2fREZV_dYkz_1g</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nMHhuYan8e3cONo3PornJA</td>\n",
       "      <td>2010-08-11</td>\n",
       "      <td>jJAIXA46pU1swYyRCdfXtQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Nobuo shows his unique talents with everything...</td>\n",
       "      <td>review</td>\n",
       "      <td>sUNkXg8-KFtCMQDV6zRzQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AsSCv0q_BWqIe3mX2JqsOQ</td>\n",
       "      <td>2010-06-16</td>\n",
       "      <td>E11jzpKz9Kw5K7fuARWfRw</td>\n",
       "      <td>5</td>\n",
       "      <td>The oldish man who owns the store is as sweet ...</td>\n",
       "      <td>review</td>\n",
       "      <td>-OMlS6yWkYjVldNhC31wYg</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e9nN4XxjdHj4qtKCOPq_vg</td>\n",
       "      <td>2011-10-21</td>\n",
       "      <td>3rPt0LxF7rgmEUrznoH22w</td>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful Vietnamese sandwich shoppe. Their ba...</td>\n",
       "      <td>review</td>\n",
       "      <td>C1rHp3dmepNea7XiouwB6Q</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>h53YuCiIDfEFSJCQpk8v1g</td>\n",
       "      <td>2010-01-11</td>\n",
       "      <td>cGnKNX3I9rthE0-TH24-qA</td>\n",
       "      <td>5</td>\n",
       "      <td>They have a limited time thing going on right ...</td>\n",
       "      <td>review</td>\n",
       "      <td>UPtysDF6cUDUxq2KY-6Dcg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>O510Re68mOy9dU490JTKCg</td>\n",
       "      <td>2010-05-03</td>\n",
       "      <td>j4SIzrIy0WrmW4yr4--Khg</td>\n",
       "      <td>5</td>\n",
       "      <td>okay this is the best place EVER! i grew up sh...</td>\n",
       "      <td>review</td>\n",
       "      <td>u1KWcbPMvXFEEYkZZ0Yktg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tdcjXyFLMKAsvRhURNOkCg</td>\n",
       "      <td>2011-06-28</td>\n",
       "      <td>LmuKVFh03Uz318VKnUWrxA</td>\n",
       "      <td>5</td>\n",
       "      <td>This place shouldn't even be reviewed - becaus...</td>\n",
       "      <td>review</td>\n",
       "      <td>YN3ZLOdg8kpnfbVcIhuEZA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>eFA9dqXT5EA_TrMgbo03QQ</td>\n",
       "      <td>2011-07-13</td>\n",
       "      <td>CQYc8hgKxV4enApDkx0IhA</td>\n",
       "      <td>5</td>\n",
       "      <td>first time my friend and I went there... it wa...</td>\n",
       "      <td>review</td>\n",
       "      <td>6lg55RIP23VhjYEBXJ8Njw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>IJ0o6b8bJFAbG6MjGfBebQ</td>\n",
       "      <td>2010-09-05</td>\n",
       "      <td>Dx9sfFU6Zn0GYOckijom-g</td>\n",
       "      <td>1</td>\n",
       "      <td>U can go there n check the car out. If u wanna...</td>\n",
       "      <td>review</td>\n",
       "      <td>zRlQEDYd_HKp0VS3hnAffA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>JhupPnWfNlMJivnWB5druA</td>\n",
       "      <td>2011-05-22</td>\n",
       "      <td>cFtQnKzn2VDpBedy_TxlvA</td>\n",
       "      <td>5</td>\n",
       "      <td>I love this place! I have been coming here for...</td>\n",
       "      <td>review</td>\n",
       "      <td>13xj6FSvYO0rZVRv5XZp4w</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>qjmCVYkwP-HDa35jwYucbQ</td>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>kZ4TzrVX6qeF0OvrVTGVEw</td>\n",
       "      <td>5</td>\n",
       "      <td>I love love LOVE this place. My boss (who is i...</td>\n",
       "      <td>review</td>\n",
       "      <td>fpItLlgimq0nRltWOkuJJw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>V1nEpIRmEa1768oj_tuxeQ</td>\n",
       "      <td>2011-05-09</td>\n",
       "      <td>dtpJXC5p_sdWDLSobluJ3Q</td>\n",
       "      <td>5</td>\n",
       "      <td>Disclaimer: Like many of you, I am a sucker fo...</td>\n",
       "      <td>review</td>\n",
       "      <td>bCKjygWJZOQHCOzootbvow</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>vvA3fbps4F9nGlAEYKk_sA</td>\n",
       "      <td>2012-05-04</td>\n",
       "      <td>S9OVpXat8k5YwWCn6FAgXg</td>\n",
       "      <td>1</td>\n",
       "      <td>Disgusting!  Had a Groupon so my daughter and ...</td>\n",
       "      <td>review</td>\n",
       "      <td>8AMn6644NmBf96xGO3w6OA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>rxQ2PIjhAx6dgAqUalf99Q</td>\n",
       "      <td>2012-09-09</td>\n",
       "      <td>-v-shjbxoj7hpU62yn6vag</td>\n",
       "      <td>5</td>\n",
       "      <td>Never having dealt with a Discount Tire in Pho...</td>\n",
       "      <td>review</td>\n",
       "      <td>HLbhD2OyiMCUDRR4c1iXaw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>o1GIYYZJjM6nM03fQs_uEQ</td>\n",
       "      <td>2011-11-30</td>\n",
       "      <td>ApKbwpYJdnhhgP4NbjQw2Q</td>\n",
       "      <td>1</td>\n",
       "      <td>I've eaten here many times, but none as bad as...</td>\n",
       "      <td>review</td>\n",
       "      <td>iwUN95LIaEr75TZE_JC6bg</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>aRkYtXfmEKYG-eTDf_qUsw</td>\n",
       "      <td>2009-04-04</td>\n",
       "      <td>Ckk1Cne1GHwzmJfo7M4r2w</td>\n",
       "      <td>5</td>\n",
       "      <td>(Un)fortunately for me, lux is close to my hou...</td>\n",
       "      <td>review</td>\n",
       "      <td>IUWjTmXc3wLVaMHz33inaA</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>zp713qNhx8d9KCJJnrw1xA</td>\n",
       "      <td>2010-02-01</td>\n",
       "      <td>Bmt1QRDT0GfyXkhOvj_BfQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Fred M. pretty much said what I would say, so ...</td>\n",
       "      <td>review</td>\n",
       "      <td>aqqbh1NZoFk48kp0eqSLdg</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>K8pM6qQdYu5h6buRE1-_sw</td>\n",
       "      <td>2009-08-06</td>\n",
       "      <td>GOconNmWgg6cJbgwSwrozw</td>\n",
       "      <td>5</td>\n",
       "      <td>Alright, I have been away from Yelp for quite ...</td>\n",
       "      <td>review</td>\n",
       "      <td>yy1SbjeyWiAhWfSr5d0new</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>E6DnUFy3GoN4DxTqturtug</td>\n",
       "      <td>2010-09-28</td>\n",
       "      <td>nzSOTOZiAb1ITqbRgWKnnQ</td>\n",
       "      <td>5</td>\n",
       "      <td>This restaurant is incredible, and has the bes...</td>\n",
       "      <td>review</td>\n",
       "      <td>U3uT-Phb8iL2iuZpAROZlg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>l4vBbCL9QbGiwLuLKwD_bA</td>\n",
       "      <td>2011-11-22</td>\n",
       "      <td>DJVxOfj2Rw9zklC9tU3i1w</td>\n",
       "      <td>1</td>\n",
       "      <td>I have always been a fan of Burlington's deals...</td>\n",
       "      <td>review</td>\n",
       "      <td>EPROVap0M19Y6_4uf3eCmQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>CEswyP-9SsXRNLR9fFGKKw</td>\n",
       "      <td>2012-05-19</td>\n",
       "      <td>GXj4PNAi095-q9ynPYH3kg</td>\n",
       "      <td>1</td>\n",
       "      <td>Another night meeting friends here.  I have to...</td>\n",
       "      <td>review</td>\n",
       "      <td>MjLAe48XNfYlTeFYca5gMw</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6Ry-gjGqApTSRZkfdYlLmw</td>\n",
       "      <td>2011-05-04</td>\n",
       "      <td>LN4l4wklQB0IVBjm-vq9TA</td>\n",
       "      <td>1</td>\n",
       "      <td>Not busy at all but took nearly 45 min to get ...</td>\n",
       "      <td>review</td>\n",
       "      <td>rWN1pg3mCF2btJWtG6JmJA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>taSsiZAGZSz2gRg7_M4XDw</td>\n",
       "      <td>2011-12-30</td>\n",
       "      <td>L_q9kLaGo33j2Ij8yUIGDw</td>\n",
       "      <td>5</td>\n",
       "      <td>This an incredible church that embraces the pr...</td>\n",
       "      <td>review</td>\n",
       "      <td>0jJhjgr3wPin-8vYnPab6A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>3oLy0rlzRI_xiqfQHqC4_g</td>\n",
       "      <td>2011-04-28</td>\n",
       "      <td>dY_p1YkjZxJmREb9Lfc5vw</td>\n",
       "      <td>5</td>\n",
       "      <td>This is our favorite breakfast place. The food...</td>\n",
       "      <td>review</td>\n",
       "      <td>-yg_AMAU2HNh48zcuQ13mw</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>N5iW2JG5e-QyUh7brRNSfw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>-_hed9F2kUDdb4oa1CXKIA</td>\n",
       "      <td>5</td>\n",
       "      <td>I had looked at several invitation websites al...</td>\n",
       "      <td>review</td>\n",
       "      <td>xP3o3-r0KRUnZqBSdIGJjw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>NNGJQF3WeIHzGzweCpZ-VA</td>\n",
       "      <td>2011-04-23</td>\n",
       "      <td>jNewOjPp56NMDrKlajrgFQ</td>\n",
       "      <td>1</td>\n",
       "      <td>Yikes, reading other reviews I realize my bad ...</td>\n",
       "      <td>review</td>\n",
       "      <td>5je-Jg8tq5BWxJtDBmTK2Q</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9941</th>\n",
       "      <td>W8WyVVxinyRjzP8gJa7ILg</td>\n",
       "      <td>2009-01-20</td>\n",
       "      <td>lgrfMIvhkCzclg5KEharjw</td>\n",
       "      <td>5</td>\n",
       "      <td>I have a fond place in my heart for this estab...</td>\n",
       "      <td>review</td>\n",
       "      <td>PShy2RYNadDUhJf4ErOJ7w</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>iV7D7fHKb-bF9fCL_bEMtA</td>\n",
       "      <td>2009-04-22</td>\n",
       "      <td>WjkBCWy7pu4U2-3PbvM0bg</td>\n",
       "      <td>5</td>\n",
       "      <td>Cork is an enigma.\\n\\nWhat makes it enigmatic ...</td>\n",
       "      <td>review</td>\n",
       "      <td>l81ILmOhky5bG7o4r3rkhQ</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9943</th>\n",
       "      <td>OY76JYsWn1rhb6Te2BE5Rw</td>\n",
       "      <td>2011-08-19</td>\n",
       "      <td>9IhdGx6Z4OUYXW9oPrPuXQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Went to Yogurt Kingdom for the first time toni...</td>\n",
       "      <td>review</td>\n",
       "      <td>haYHbLXLX2cmHq1wQAEtfg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9945</th>\n",
       "      <td>hfl62LX14YqNpG0g0Tj6_Q</td>\n",
       "      <td>2012-02-27</td>\n",
       "      <td>2hvg6T-sCh7upAe17Jxs3w</td>\n",
       "      <td>5</td>\n",
       "      <td>I find it hilarious that someone would referen...</td>\n",
       "      <td>review</td>\n",
       "      <td>9g_ARKx0eJmNH8oO1TR9rQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9947</th>\n",
       "      <td>VsrvWdZL2993olnW3z8R3A</td>\n",
       "      <td>2011-08-20</td>\n",
       "      <td>GW5D1Dv1Uc8X0_-yf0pzPA</td>\n",
       "      <td>5</td>\n",
       "      <td>LOVE Five Guys!</td>\n",
       "      <td>review</td>\n",
       "      <td>FYdLG8vxUfhqd6v-QQHRkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9951</th>\n",
       "      <td>EGiGWZp_zSqdeftiFQ7MbA</td>\n",
       "      <td>2011-06-13</td>\n",
       "      <td>OvsDIpdK5T4G81NUzWAkTA</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a great Mexican food restaurant. I eat...</td>\n",
       "      <td>review</td>\n",
       "      <td>fi5y6-daDzTIQ7wOKaR7xg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9953</th>\n",
       "      <td>QCxXYA13PtkD3wec47_r8Q</td>\n",
       "      <td>2012-03-14</td>\n",
       "      <td>V2N2TCqnaqMYfg7zXGG-Rg</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Hipster,Trendy\" ????-I think NOT !!!! Very di...</td>\n",
       "      <td>review</td>\n",
       "      <td>JEQ6el2-tLtKJU6k_SpE-w</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9956</th>\n",
       "      <td>7tPe20uDErh-iSkfNEWzVQ</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>b0NnnjzhDmI7feVNzniZ3w</td>\n",
       "      <td>5</td>\n",
       "      <td>\"So Jimmy, tell the class what you saw at Swee...</td>\n",
       "      <td>review</td>\n",
       "      <td>P2kVk4cIWyK4e4h14RhK-Q</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9957</th>\n",
       "      <td>62F17L8z4Q4S7U_TayuDBA</td>\n",
       "      <td>2010-03-13</td>\n",
       "      <td>oTm0bBYcbgoMPJloZUpUwQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Standard Mexican fare - but quite delicious.  ...</td>\n",
       "      <td>review</td>\n",
       "      <td>TnTkd3MKoIOKqrsPDzhiog</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9959</th>\n",
       "      <td>uEJQSIjWui-TDWXaGlcqyQ</td>\n",
       "      <td>2010-09-01</td>\n",
       "      <td>-2oWwvRjNEHNDa4TWVlbAA</td>\n",
       "      <td>5</td>\n",
       "      <td>My profile says....\\n\\nMy Last Meal On Earth: ...</td>\n",
       "      <td>review</td>\n",
       "      <td>EDN_wou8EEkuaj5Pd83cQA</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9965</th>\n",
       "      <td>L9UYbtAUOcfTgZFimehlXw</td>\n",
       "      <td>2009-07-31</td>\n",
       "      <td>F9KzEFjAtbWr3h8toNtDgw</td>\n",
       "      <td>5</td>\n",
       "      <td>Treats: We tried the cookies (chocolate chip a...</td>\n",
       "      <td>review</td>\n",
       "      <td>2lvfBteL5ny4CBjNIk9Mwg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9966</th>\n",
       "      <td>ttpZx2t4fMAApdU9MFG91w</td>\n",
       "      <td>2012-07-20</td>\n",
       "      <td>rR322HOBSV2JSY6omtNoPw</td>\n",
       "      <td>5</td>\n",
       "      <td>I first joined 24 hr fitness about a year ago,...</td>\n",
       "      <td>review</td>\n",
       "      <td>iPnpD62l5LUL7WXaI3SJFA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9969</th>\n",
       "      <td>qhIlkXgcC4j34lNTIqu9WA</td>\n",
       "      <td>2011-03-16</td>\n",
       "      <td>qu7tpZFxaPSxiuIgFiiYlQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Leah, the trainer, at Dog House Training Acade...</td>\n",
       "      <td>review</td>\n",
       "      <td>RKxsZo6XkRU-0y1JgqqsBA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>R6aazv8FB-6BeanY3ag8kw</td>\n",
       "      <td>2009-09-26</td>\n",
       "      <td>gP17ykqduf3AlewSaRb61w</td>\n",
       "      <td>5</td>\n",
       "      <td>This place is super cute lunch joint.  I had t...</td>\n",
       "      <td>review</td>\n",
       "      <td>mtoKqaQjGPWEc5YZbrYV9w</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>JOZqBKIOB8WEBAWm7v1JFA</td>\n",
       "      <td>2008-07-22</td>\n",
       "      <td>QI9rfeWrZnvK5ojz8cEoRg</td>\n",
       "      <td>5</td>\n",
       "      <td>The staff is great, the food is great, even th...</td>\n",
       "      <td>review</td>\n",
       "      <td>uBAMd01ZtGXaHrRD6THNzg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>XHr5mXFgobOHoxbPJxmYdg</td>\n",
       "      <td>2009-09-28</td>\n",
       "      <td>udMiWjeG0OGcb4nNddDkBg</td>\n",
       "      <td>5</td>\n",
       "      <td>Wow!  Went on a Sunday around 11am - busy but ...</td>\n",
       "      <td>review</td>\n",
       "      <td>yRYNx24kUDRRBfJu1Rcojg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>EWMwV5V9BxNs_U6nNVMeqw</td>\n",
       "      <td>2007-10-20</td>\n",
       "      <td>g4LsVAoafmUDHiS-_yN4tA</td>\n",
       "      <td>5</td>\n",
       "      <td>When I lived in Phoenix, I was a regular at Fe...</td>\n",
       "      <td>review</td>\n",
       "      <td>TLj3XaclA7V4ldJ5yNP-9Q</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>iDYzGVIF1TDWdjHNgNjCVw</td>\n",
       "      <td>2012-10-30</td>\n",
       "      <td>qaNZyCUJA6Yp0mvPBCknPQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Why did I wait so long to try this neighborhoo...</td>\n",
       "      <td>review</td>\n",
       "      <td>Id-8-NMEKxeXBR44eUdDeA</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>9Y3aQAVITkEJYe5vLZr13w</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>ZoTUU6EJ1OBNr7mhqxHBLw</td>\n",
       "      <td>5</td>\n",
       "      <td>This is the place for a fabulos breakfast!! I ...</td>\n",
       "      <td>review</td>\n",
       "      <td>vasHsAZEgLZGJDTlIweUYQ</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>GV1P1x9eRb4iZHCxj5_IjA</td>\n",
       "      <td>2012-12-07</td>\n",
       "      <td>eVUs1C4yaVJNrc7SGTAheg</td>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommend. This is my second time here ...</td>\n",
       "      <td>review</td>\n",
       "      <td>bJFdmJJxfXgCYA5DMmyeqQ</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>GHYOl_cnERMOhkCK_mGAlA</td>\n",
       "      <td>2011-07-03</td>\n",
       "      <td>Q-y3jSqccdytKxAyo1J0Xg</td>\n",
       "      <td>5</td>\n",
       "      <td>5 stars for the great $5 happy hour specials. ...</td>\n",
       "      <td>review</td>\n",
       "      <td>xZvRLPJ1ixhFVomkXSfXAw</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>AX8lx9wHNYT45lyd7pxaYw</td>\n",
       "      <td>2008-11-27</td>\n",
       "      <td>IyunTh7jnG7v3EYwfF3hPw</td>\n",
       "      <td>5</td>\n",
       "      <td>We brought the entire family to Giuseppe's las...</td>\n",
       "      <td>review</td>\n",
       "      <td>fczQCSmaWF78toLEmb0Zsw</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>wepFVY82q_tuDzG6lQjHWw</td>\n",
       "      <td>2012-02-12</td>\n",
       "      <td>spusZYROtBKw_5tv3gYm4Q</td>\n",
       "      <td>1</td>\n",
       "      <td>Went last night to Whore Foods to get basics t...</td>\n",
       "      <td>review</td>\n",
       "      <td>W7zmm1uzlyUkEqpSG7PlBw</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>r-a-Cn9hxdEnYTtVTB5bMQ</td>\n",
       "      <td>2012-04-07</td>\n",
       "      <td>j9HwZZoBBmJgOlqDSuJcxg</td>\n",
       "      <td>1</td>\n",
       "      <td>The food is delicious.  The service:  discrimi...</td>\n",
       "      <td>review</td>\n",
       "      <td>toPtsUtYoRB-5-ThrOy2Fg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>mQUC-ATrFuMQSaDQb93Pug</td>\n",
       "      <td>2011-10-01</td>\n",
       "      <td>ta2P9joJqeFB8BzFp-AzjA</td>\n",
       "      <td>5</td>\n",
       "      <td>Great food and service! Country food at its best!</td>\n",
       "      <td>review</td>\n",
       "      <td>fKaO8fR1IAcfvZb6cBrs2w</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>R8VwdLyvsp9iybNqRvm94g</td>\n",
       "      <td>2011-10-03</td>\n",
       "      <td>pcEeHdAJPoFNF23es0kKWg</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes I do rock the hipster joints.  I dig this ...</td>\n",
       "      <td>review</td>\n",
       "      <td>b92Y3tyWTQQZ5FLifex62Q</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>WJ5mq4EiWYAA4Vif0xDfdg</td>\n",
       "      <td>2011-12-05</td>\n",
       "      <td>EuHX-39FR7tyyG1ElvN1Jw</td>\n",
       "      <td>5</td>\n",
       "      <td>Only 4 stars? \\n\\n(A few notes: The folks that...</td>\n",
       "      <td>review</td>\n",
       "      <td>hTau-iNZFwoNsPCaiIUTEA</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>f96lWMIAUhYIYy9gOktivQ</td>\n",
       "      <td>2009-03-10</td>\n",
       "      <td>YF17z7HWlMj6aezZc-pVEw</td>\n",
       "      <td>5</td>\n",
       "      <td>I'm not normally one to jump at reviewing a ch...</td>\n",
       "      <td>review</td>\n",
       "      <td>W_QXYA7A0IhMrvbckz7eVg</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>L3BSpFvxcNf3T_teitgt6A</td>\n",
       "      <td>2012-03-19</td>\n",
       "      <td>0nxb1gIGFgk3WbC5zwhKZg</td>\n",
       "      <td>5</td>\n",
       "      <td>Let's see...what is there NOT to like about Su...</td>\n",
       "      <td>review</td>\n",
       "      <td>OzOZv-Knlw3oz9K5Kh5S6A</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>pF7uRzygyZsltbmVpjIyvw</td>\n",
       "      <td>2010-10-16</td>\n",
       "      <td>vWSmOhg2ID1MNZHaWapGbA</td>\n",
       "      <td>5</td>\n",
       "      <td>4-5 locations.. all 4.5 star average.. I think...</td>\n",
       "      <td>review</td>\n",
       "      <td>KSBFytcdjPKZgXKQnYQdkA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4086 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 business_id        date               review_id  stars  \\\n",
       "0     9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1     ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "3     _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4     6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "6     zp713qNhx8d9KCJJnrw1xA  2010-02-12  riFQ3vxNpP4rWLk_CSri2A      5   \n",
       "9     nMHhuYan8e3cONo3PornJA  2010-08-11  jJAIXA46pU1swYyRCdfXtQ      5   \n",
       "10    AsSCv0q_BWqIe3mX2JqsOQ  2010-06-16  E11jzpKz9Kw5K7fuARWfRw      5   \n",
       "11    e9nN4XxjdHj4qtKCOPq_vg  2011-10-21  3rPt0LxF7rgmEUrznoH22w      5   \n",
       "12    h53YuCiIDfEFSJCQpk8v1g  2010-01-11  cGnKNX3I9rthE0-TH24-qA      5   \n",
       "17    O510Re68mOy9dU490JTKCg  2010-05-03  j4SIzrIy0WrmW4yr4--Khg      5   \n",
       "21    tdcjXyFLMKAsvRhURNOkCg  2011-06-28  LmuKVFh03Uz318VKnUWrxA      5   \n",
       "22    eFA9dqXT5EA_TrMgbo03QQ  2011-07-13  CQYc8hgKxV4enApDkx0IhA      5   \n",
       "23    IJ0o6b8bJFAbG6MjGfBebQ  2010-09-05  Dx9sfFU6Zn0GYOckijom-g      1   \n",
       "24    JhupPnWfNlMJivnWB5druA  2011-05-22  cFtQnKzn2VDpBedy_TxlvA      5   \n",
       "26    qjmCVYkwP-HDa35jwYucbQ  2013-01-03  kZ4TzrVX6qeF0OvrVTGVEw      5   \n",
       "30    V1nEpIRmEa1768oj_tuxeQ  2011-05-09  dtpJXC5p_sdWDLSobluJ3Q      5   \n",
       "31    vvA3fbps4F9nGlAEYKk_sA  2012-05-04  S9OVpXat8k5YwWCn6FAgXg      1   \n",
       "32    rxQ2PIjhAx6dgAqUalf99Q  2012-09-09  -v-shjbxoj7hpU62yn6vag      5   \n",
       "35    o1GIYYZJjM6nM03fQs_uEQ  2011-11-30  ApKbwpYJdnhhgP4NbjQw2Q      1   \n",
       "46    aRkYtXfmEKYG-eTDf_qUsw  2009-04-04  Ckk1Cne1GHwzmJfo7M4r2w      5   \n",
       "51    zp713qNhx8d9KCJJnrw1xA  2010-02-01  Bmt1QRDT0GfyXkhOvj_BfQ      5   \n",
       "54    K8pM6qQdYu5h6buRE1-_sw  2009-08-06  GOconNmWgg6cJbgwSwrozw      5   \n",
       "59    E6DnUFy3GoN4DxTqturtug  2010-09-28  nzSOTOZiAb1ITqbRgWKnnQ      5   \n",
       "61    l4vBbCL9QbGiwLuLKwD_bA  2011-11-22  DJVxOfj2Rw9zklC9tU3i1w      1   \n",
       "64    CEswyP-9SsXRNLR9fFGKKw  2012-05-19  GXj4PNAi095-q9ynPYH3kg      1   \n",
       "65    6Ry-gjGqApTSRZkfdYlLmw  2011-05-04  LN4l4wklQB0IVBjm-vq9TA      1   \n",
       "66    taSsiZAGZSz2gRg7_M4XDw  2011-12-30  L_q9kLaGo33j2Ij8yUIGDw      5   \n",
       "67    3oLy0rlzRI_xiqfQHqC4_g  2011-04-28  dY_p1YkjZxJmREb9Lfc5vw      5   \n",
       "69    N5iW2JG5e-QyUh7brRNSfw  2012-01-05  -_hed9F2kUDdb4oa1CXKIA      5   \n",
       "71    NNGJQF3WeIHzGzweCpZ-VA  2011-04-23  jNewOjPp56NMDrKlajrgFQ      1   \n",
       "...                      ...         ...                     ...    ...   \n",
       "9941  W8WyVVxinyRjzP8gJa7ILg  2009-01-20  lgrfMIvhkCzclg5KEharjw      5   \n",
       "9942  iV7D7fHKb-bF9fCL_bEMtA  2009-04-22  WjkBCWy7pu4U2-3PbvM0bg      5   \n",
       "9943  OY76JYsWn1rhb6Te2BE5Rw  2011-08-19  9IhdGx6Z4OUYXW9oPrPuXQ      5   \n",
       "9945  hfl62LX14YqNpG0g0Tj6_Q  2012-02-27  2hvg6T-sCh7upAe17Jxs3w      5   \n",
       "9947  VsrvWdZL2993olnW3z8R3A  2011-08-20  GW5D1Dv1Uc8X0_-yf0pzPA      5   \n",
       "9951  EGiGWZp_zSqdeftiFQ7MbA  2011-06-13  OvsDIpdK5T4G81NUzWAkTA      5   \n",
       "9953  QCxXYA13PtkD3wec47_r8Q  2012-03-14  V2N2TCqnaqMYfg7zXGG-Rg      1   \n",
       "9956  7tPe20uDErh-iSkfNEWzVQ  2011-01-26  b0NnnjzhDmI7feVNzniZ3w      5   \n",
       "9957  62F17L8z4Q4S7U_TayuDBA  2010-03-13  oTm0bBYcbgoMPJloZUpUwQ      5   \n",
       "9959  uEJQSIjWui-TDWXaGlcqyQ  2010-09-01  -2oWwvRjNEHNDa4TWVlbAA      5   \n",
       "9965  L9UYbtAUOcfTgZFimehlXw  2009-07-31  F9KzEFjAtbWr3h8toNtDgw      5   \n",
       "9966  ttpZx2t4fMAApdU9MFG91w  2012-07-20  rR322HOBSV2JSY6omtNoPw      5   \n",
       "9969  qhIlkXgcC4j34lNTIqu9WA  2011-03-16  qu7tpZFxaPSxiuIgFiiYlQ      5   \n",
       "9970  R6aazv8FB-6BeanY3ag8kw  2009-09-26  gP17ykqduf3AlewSaRb61w      5   \n",
       "9971  JOZqBKIOB8WEBAWm7v1JFA  2008-07-22  QI9rfeWrZnvK5ojz8cEoRg      5   \n",
       "9973  XHr5mXFgobOHoxbPJxmYdg  2009-09-28  udMiWjeG0OGcb4nNddDkBg      5   \n",
       "9975  EWMwV5V9BxNs_U6nNVMeqw  2007-10-20  g4LsVAoafmUDHiS-_yN4tA      5   \n",
       "9977  iDYzGVIF1TDWdjHNgNjCVw  2012-10-30  qaNZyCUJA6Yp0mvPBCknPQ      5   \n",
       "9978  9Y3aQAVITkEJYe5vLZr13w  2010-04-01  ZoTUU6EJ1OBNr7mhqxHBLw      5   \n",
       "9979  GV1P1x9eRb4iZHCxj5_IjA  2012-12-07  eVUs1C4yaVJNrc7SGTAheg      5   \n",
       "9980  GHYOl_cnERMOhkCK_mGAlA  2011-07-03  Q-y3jSqccdytKxAyo1J0Xg      5   \n",
       "9981  AX8lx9wHNYT45lyd7pxaYw  2008-11-27  IyunTh7jnG7v3EYwfF3hPw      5   \n",
       "9984  wepFVY82q_tuDzG6lQjHWw  2012-02-12  spusZYROtBKw_5tv3gYm4Q      1   \n",
       "9987  r-a-Cn9hxdEnYTtVTB5bMQ  2012-04-07  j9HwZZoBBmJgOlqDSuJcxg      1   \n",
       "9989  mQUC-ATrFuMQSaDQb93Pug  2011-10-01  ta2P9joJqeFB8BzFp-AzjA      5   \n",
       "9990  R8VwdLyvsp9iybNqRvm94g  2011-10-03  pcEeHdAJPoFNF23es0kKWg      5   \n",
       "9991  WJ5mq4EiWYAA4Vif0xDfdg  2011-12-05  EuHX-39FR7tyyG1ElvN1Jw      5   \n",
       "9992  f96lWMIAUhYIYy9gOktivQ  2009-03-10  YF17z7HWlMj6aezZc-pVEw      5   \n",
       "9994  L3BSpFvxcNf3T_teitgt6A  2012-03-19  0nxb1gIGFgk3WbC5zwhKZg      5   \n",
       "9999  pF7uRzygyZsltbmVpjIyvw  2010-10-16  vWSmOhg2ID1MNZHaWapGbA      5   \n",
       "\n",
       "                                                   text    type  \\\n",
       "0     My wife took me here on my birthday for breakf...  review   \n",
       "1     I have no idea why some people give bad review...  review   \n",
       "3     Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4     General Manager Scott Petello is a good egg!!!...  review   \n",
       "6     Drop what you're doing and drive here. After I...  review   \n",
       "9     Nobuo shows his unique talents with everything...  review   \n",
       "10    The oldish man who owns the store is as sweet ...  review   \n",
       "11    Wonderful Vietnamese sandwich shoppe. Their ba...  review   \n",
       "12    They have a limited time thing going on right ...  review   \n",
       "17    okay this is the best place EVER! i grew up sh...  review   \n",
       "21    This place shouldn't even be reviewed - becaus...  review   \n",
       "22    first time my friend and I went there... it wa...  review   \n",
       "23    U can go there n check the car out. If u wanna...  review   \n",
       "24    I love this place! I have been coming here for...  review   \n",
       "26    I love love LOVE this place. My boss (who is i...  review   \n",
       "30    Disclaimer: Like many of you, I am a sucker fo...  review   \n",
       "31    Disgusting!  Had a Groupon so my daughter and ...  review   \n",
       "32    Never having dealt with a Discount Tire in Pho...  review   \n",
       "35    I've eaten here many times, but none as bad as...  review   \n",
       "46    (Un)fortunately for me, lux is close to my hou...  review   \n",
       "51    Fred M. pretty much said what I would say, so ...  review   \n",
       "54    Alright, I have been away from Yelp for quite ...  review   \n",
       "59    This restaurant is incredible, and has the bes...  review   \n",
       "61    I have always been a fan of Burlington's deals...  review   \n",
       "64    Another night meeting friends here.  I have to...  review   \n",
       "65    Not busy at all but took nearly 45 min to get ...  review   \n",
       "66    This an incredible church that embraces the pr...  review   \n",
       "67    This is our favorite breakfast place. The food...  review   \n",
       "69    I had looked at several invitation websites al...  review   \n",
       "71    Yikes, reading other reviews I realize my bad ...  review   \n",
       "...                                                 ...     ...   \n",
       "9941  I have a fond place in my heart for this estab...  review   \n",
       "9942  Cork is an enigma.\\n\\nWhat makes it enigmatic ...  review   \n",
       "9943  Went to Yogurt Kingdom for the first time toni...  review   \n",
       "9945  I find it hilarious that someone would referen...  review   \n",
       "9947                                    LOVE Five Guys!  review   \n",
       "9951  This is a great Mexican food restaurant. I eat...  review   \n",
       "9953  \"Hipster,Trendy\" ????-I think NOT !!!! Very di...  review   \n",
       "9956  \"So Jimmy, tell the class what you saw at Swee...  review   \n",
       "9957  Standard Mexican fare - but quite delicious.  ...  review   \n",
       "9959  My profile says....\\n\\nMy Last Meal On Earth: ...  review   \n",
       "9965  Treats: We tried the cookies (chocolate chip a...  review   \n",
       "9966  I first joined 24 hr fitness about a year ago,...  review   \n",
       "9969  Leah, the trainer, at Dog House Training Acade...  review   \n",
       "9970  This place is super cute lunch joint.  I had t...  review   \n",
       "9971  The staff is great, the food is great, even th...  review   \n",
       "9973  Wow!  Went on a Sunday around 11am - busy but ...  review   \n",
       "9975  When I lived in Phoenix, I was a regular at Fe...  review   \n",
       "9977  Why did I wait so long to try this neighborhoo...  review   \n",
       "9978  This is the place for a fabulos breakfast!! I ...  review   \n",
       "9979  Highly recommend. This is my second time here ...  review   \n",
       "9980  5 stars for the great $5 happy hour specials. ...  review   \n",
       "9981  We brought the entire family to Giuseppe's las...  review   \n",
       "9984  Went last night to Whore Foods to get basics t...  review   \n",
       "9987  The food is delicious.  The service:  discrimi...  review   \n",
       "9989  Great food and service! Country food at its best!  review   \n",
       "9990  Yes I do rock the hipster joints.  I dig this ...  review   \n",
       "9991  Only 4 stars? \\n\\n(A few notes: The folks that...  review   \n",
       "9992  I'm not normally one to jump at reviewing a ch...  review   \n",
       "9994  Let's see...what is there NOT to like about Su...  review   \n",
       "9999  4-5 locations.. all 4.5 star average.. I think...  review   \n",
       "\n",
       "                     user_id  cool  useful  funny  \n",
       "0     rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1     0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "3     uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4     vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  \n",
       "6     wFweIWhv2fREZV_dYkz_1g     7       7      4  \n",
       "9     sUNkXg8-KFtCMQDV6zRzQg     0       1      0  \n",
       "10    -OMlS6yWkYjVldNhC31wYg     1       3      1  \n",
       "11    C1rHp3dmepNea7XiouwB6Q     1       1      0  \n",
       "12    UPtysDF6cUDUxq2KY-6Dcg     1       2      0  \n",
       "17    u1KWcbPMvXFEEYkZZ0Yktg     0       0      0  \n",
       "21    YN3ZLOdg8kpnfbVcIhuEZA     1       1      2  \n",
       "22    6lg55RIP23VhjYEBXJ8Njw     0       0      0  \n",
       "23    zRlQEDYd_HKp0VS3hnAffA     0       1      1  \n",
       "24    13xj6FSvYO0rZVRv5XZp4w     0       1      0  \n",
       "26    fpItLlgimq0nRltWOkuJJw     0       0      0  \n",
       "30    bCKjygWJZOQHCOzootbvow     0       2      0  \n",
       "31    8AMn6644NmBf96xGO3w6OA     0       1      0  \n",
       "32    HLbhD2OyiMCUDRR4c1iXaw     0       0      0  \n",
       "35    iwUN95LIaEr75TZE_JC6bg     0       4      3  \n",
       "46    IUWjTmXc3wLVaMHz33inaA     2       1      1  \n",
       "51    aqqbh1NZoFk48kp0eqSLdg     3       2      1  \n",
       "54    yy1SbjeyWiAhWfSr5d0new     2       3      3  \n",
       "59    U3uT-Phb8iL2iuZpAROZlg     0       0      0  \n",
       "61    EPROVap0M19Y6_4uf3eCmQ     0       0      0  \n",
       "64    MjLAe48XNfYlTeFYca5gMw     0       1      2  \n",
       "65    rWN1pg3mCF2btJWtG6JmJA     0       1      0  \n",
       "66    0jJhjgr3wPin-8vYnPab6A     0       0      0  \n",
       "67    -yg_AMAU2HNh48zcuQ13mw     0       1      0  \n",
       "69    xP3o3-r0KRUnZqBSdIGJjw     0       0      0  \n",
       "71    5je-Jg8tq5BWxJtDBmTK2Q     0       6      1  \n",
       "...                      ...   ...     ...    ...  \n",
       "9941  PShy2RYNadDUhJf4ErOJ7w     1       1      3  \n",
       "9942  l81ILmOhky5bG7o4r3rkhQ     8      12      6  \n",
       "9943  haYHbLXLX2cmHq1wQAEtfg     0       0      0  \n",
       "9945  9g_ARKx0eJmNH8oO1TR9rQ     0       0      1  \n",
       "9947  FYdLG8vxUfhqd6v-QQHRkw     0       0      0  \n",
       "9951  fi5y6-daDzTIQ7wOKaR7xg     0       0      0  \n",
       "9953  JEQ6el2-tLtKJU6k_SpE-w     0       2      0  \n",
       "9956  P2kVk4cIWyK4e4h14RhK-Q    14      15     16  \n",
       "9957  TnTkd3MKoIOKqrsPDzhiog     0       0      0  \n",
       "9959  EDN_wou8EEkuaj5Pd83cQA     2       2      1  \n",
       "9965  2lvfBteL5ny4CBjNIk9Mwg     0       0      0  \n",
       "9966  iPnpD62l5LUL7WXaI3SJFA     0       0      1  \n",
       "9969  RKxsZo6XkRU-0y1JgqqsBA     1       1      0  \n",
       "9970  mtoKqaQjGPWEc5YZbrYV9w     0       0      0  \n",
       "9971  uBAMd01ZtGXaHrRD6THNzg     1       2      1  \n",
       "9973  yRYNx24kUDRRBfJu1Rcojg     0       0      0  \n",
       "9975  TLj3XaclA7V4ldJ5yNP-9Q     1       1      0  \n",
       "9977  Id-8-NMEKxeXBR44eUdDeA     3       6      3  \n",
       "9978  vasHsAZEgLZGJDTlIweUYQ     0       1      0  \n",
       "9979  bJFdmJJxfXgCYA5DMmyeqQ     2       2      1  \n",
       "9980  xZvRLPJ1ixhFVomkXSfXAw     6       6      4  \n",
       "9981  fczQCSmaWF78toLEmb0Zsw    10       9      5  \n",
       "9984  W7zmm1uzlyUkEqpSG7PlBw     0       1      2  \n",
       "9987  toPtsUtYoRB-5-ThrOy2Fg     0       0      0  \n",
       "9989  fKaO8fR1IAcfvZb6cBrs2w     0       1      0  \n",
       "9990  b92Y3tyWTQQZ5FLifex62Q     1       1      1  \n",
       "9991  hTau-iNZFwoNsPCaiIUTEA     1       1      0  \n",
       "9992  W_QXYA7A0IhMrvbckz7eVg     2       3      2  \n",
       "9994  OzOZv-Knlw3oz9K5Kh5S6A     1       2      1  \n",
       "9999  KSBFytcdjPKZgXKQnYQdkA     0       0      0  \n",
       "\n",
       "[4086 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_best_worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predict the star rating from text\n",
    "# Define X and y.\n",
    "\n",
    "X = yelp_best_worst.loc[:, 'text']\n",
    "y = yelp_best_worst.loc[:, 'stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the new DataFrame into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='text_class'></a>\n",
    "\n",
    "\n",
    "# Introduction: Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text classification is the task of predicting which category or topic a text sample is from.**\n",
    "\n",
    "E.g.:\n",
    "- Is an article a sports or business story?\n",
    "- Does an email have positive or negative sentiment?\n",
    "- Is the rating of a recipe 1, 2, 3, 4, or 5 stars?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Turning text into feature vectors**\n",
    "\n",
    "The only difference between this task and the kinds of classification tasks we have been considering is that our inputs consist of text rather than numeric features.\n",
    "\n",
    "If we can find a way to represent text using a set of numeric features, then we can use standard machine learning classifiers for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start out with a **bag-of-words representation:**\n",
    "\n",
    "- Preprocess the text, e.g. to remove punctuation and convert uppercase letters to lowercase.\n",
    "- Create a vocabulary, e.g. every word in the corpus.\n",
    "- Make each word in the vocabulary a feature.\n",
    "- Represent each document with a vector that indicates how many times each word in the vocabulary appears in that document.\n",
    "\n",
    "**Notes on bag-of-words:**\n",
    "\n",
    "- Vocabulary will often contain tens of thousands of words or more.\n",
    "- Most features will have a value of zero for most documents, resulting in a sparse matrix of features.\n",
    "- This approach is called \"bag-of-words\" because it loses the document's structure â€” as if the words are all jumbled up in a bag.\n",
    "- Rather than counting occurrences of each word, you might just record a 1 or 0 to indicate whether it is present or divide by the length of the document to indicate the word's frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Text Processing in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='count_vec'></a>\n",
    "### Creating Features Using CountVectorizer\n",
    "\n",
    "- **What:** Converts each document into a set of words and their counts.\n",
    "- **Why:** To use a machine learning model, we must convert unstructured text into numeric features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='countvectorizer-model'></a>\n",
    "\n",
    "\n",
    "### Using CountVectorizer in a Model\n",
    "![DTM](./images/DTM.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use CountVectorizer to create document-term matrices from X_train and X_test.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3064x16825 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 237720 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dtm = document term matrix\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3064x16825 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 237720 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformed feature matrices are stored as sparse matrices for efficiency.\n",
    "\n",
    "# A sparse representation stores the vaues and locations of non-zero elements,\n",
    "# rather than storing a number for every element, which saves space when\n",
    "# most elements are zero.\n",
    "\n",
    "X_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16825)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View as a dense matrix, it's a numpy matrix, not a dataframe\n",
    "#each row is a document, each col is a word\n",
    "#without toDense, this is giving a SparseMatrix\n",
    "# it's inefficient if every col is 0\n",
    "# more efficient is to keep track of a list of locations that are not 0's\n",
    "\n",
    "\n",
    "X_train_dtm.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16825)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rows are documents, columns are terms (aka \"tokens\" or \"features\", individual words in this situation).\n",
    "\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[65,  9,  1, ...,  1,  1,  1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3064 of what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of yelp reviews that have either 5 or 1 stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 16825 of what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of words in the vocabulary of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would you interpret the output of `X_train_dtm.sum(axis=1)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the total word count for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How would you interpret the output of `X_train_dtm.sum(axis=0)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the total of each word in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yyyyy',\n",
       " 'z11',\n",
       " 'za',\n",
       " 'zabba',\n",
       " 'zach',\n",
       " 'zam',\n",
       " 'zanella',\n",
       " 'zankou',\n",
       " 'zappos',\n",
       " 'zatsiki',\n",
       " 'zen',\n",
       " 'zero',\n",
       " 'zest',\n",
       " 'zexperience',\n",
       " 'zha',\n",
       " 'zhou',\n",
       " 'zia',\n",
       " 'zihuatenejo',\n",
       " 'zilch',\n",
       " 'zin',\n",
       " 'zinburger',\n",
       " 'zinburgergeist',\n",
       " 'zinc',\n",
       " 'zinfandel',\n",
       " 'zing',\n",
       " 'zip',\n",
       " 'zipcar',\n",
       " 'zipper',\n",
       " 'zippers',\n",
       " 'zipps',\n",
       " 'ziti',\n",
       " 'zoe',\n",
       " 'zombi',\n",
       " 'zombies',\n",
       " 'zone',\n",
       " 'zones',\n",
       " 'zoning',\n",
       " 'zoo',\n",
       " 'zoyo',\n",
       " 'zucca',\n",
       " 'zucchini',\n",
       " 'zuchinni',\n",
       " 'zumba',\n",
       " 'zupa',\n",
       " 'zuzu',\n",
       " 'zwiebel',\n",
       " 'zzed',\n",
       " 'Ã©clairs',\n",
       " 'Ã©cole',\n",
       " 'Ã©m']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last 50 features\n",
    "vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filly': 5773,\n",
       " 'only': 10362,\n",
       " 'reviews': 12465,\n",
       " 'nine': 10069,\n",
       " 'now': 10180,\n",
       " 'wow': 16612,\n",
       " 'do': 4631,\n",
       " 'miss': 9578,\n",
       " 'this': 15093,\n",
       " 'place': 11186,\n",
       " '24hrs': 136,\n",
       " 'drive': 4809,\n",
       " 'thru': 15136,\n",
       " 'or': 10413,\n",
       " 'walk': 16195,\n",
       " 'up': 15834,\n",
       " 'ridiculously': 12514,\n",
       " 'cheap': 2789,\n",
       " 'tasty': 14838,\n",
       " 'of': 10286,\n",
       " 'course': 3679,\n",
       " 'the': 15032,\n",
       " 'arizona': 1018,\n",
       " 'burritos': 2286,\n",
       " 'are': 1003,\n",
       " 'good': 6571,\n",
       " 'everything': 5342,\n",
       " 'is': 7956,\n",
       " 'used': 15885,\n",
       " 'to': 15228,\n",
       " 'love': 8899,\n",
       " 'one': 10354,\n",
       " 'combos': 3233,\n",
       " 'you': 16727,\n",
       " 'get': 6433,\n",
       " 'beef': 1564,\n",
       " 'burrito': 2285,\n",
       " 'taco': 14720,\n",
       " 'rice': 12489,\n",
       " 'and': 805,\n",
       " 'beans': 1528,\n",
       " 'for': 6028,\n",
       " 'under': 15683,\n",
       " 'color': 3213,\n",
       " 'me': 9301,\n",
       " 'silly': 13462,\n",
       " 'call': 2398,\n",
       " 'sally': 12808,\n",
       " 'they': 15067,\n",
       " 'have': 7023,\n",
       " 'bomb': 1902,\n",
       " 'horchata': 7345,\n",
       " 'too': 15281,\n",
       " 'really': 12038,\n",
       " 'fresh': 6154,\n",
       " 'flautas': 5882,\n",
       " 'rolled': 12622,\n",
       " 'tacos': 14721,\n",
       " 'breakfast': 2069,\n",
       " 'damn': 3999,\n",
       " 'here': 7149,\n",
       " 'whether': 16394,\n",
       " 'drunk': 4837,\n",
       " 'sober': 13745,\n",
       " 'my': 9868,\n",
       " 'husband': 7476,\n",
       " 'absolutely': 353,\n",
       " 'restaurant': 12410,\n",
       " 'anytime': 889,\n",
       " 'find': 5790,\n",
       " 'myself': 9871,\n",
       " 'craving': 3757,\n",
       " 'mexican': 9440,\n",
       " 'food': 6006,\n",
       " 'first': 5828,\n",
       " 'that': 15027,\n",
       " 'pops': 11347,\n",
       " 'in': 7619,\n",
       " 'head': 7042,\n",
       " 'salsa': 12814,\n",
       " 'blanca': 1780,\n",
       " 'we': 16306,\n",
       " 'always': 735,\n",
       " 'encountered': 5144,\n",
       " 'friendly': 6171,\n",
       " 'welcoming': 16355,\n",
       " 'staff': 14107,\n",
       " 'amazing': 752,\n",
       " 'fulfilling': 6251,\n",
       " 'what': 16377,\n",
       " 'more': 9735,\n",
       " 'could': 3655,\n",
       " 'ask': 1101,\n",
       " 'went': 16361,\n",
       " 'today': 15237,\n",
       " 'after': 563,\n",
       " 'lunch': 8945,\n",
       " 'got': 6605,\n",
       " 'usual': 15893,\n",
       " 'lime': 8707,\n",
       " 'basil': 1477,\n",
       " 'real': 12029,\n",
       " 'mint': 9540,\n",
       " 'chip': 2896,\n",
       " 'which': 16396,\n",
       " 'leaves': 8570,\n",
       " 'hubby': 7420,\n",
       " 'chocolate': 2914,\n",
       " 'guiness': 6820,\n",
       " 'four': 6089,\n",
       " 'peaks': 10866,\n",
       " 'hop': 7329,\n",
       " 'knot': 8348,\n",
       " 'best': 1657,\n",
       " 'ice': 7503,\n",
       " 'cream': 3766,\n",
       " 'phoenix': 11045,\n",
       " 'super': 14545,\n",
       " 'nice': 10041,\n",
       " 'give': 6481,\n",
       " 'us': 15879,\n",
       " 'bags': 1345,\n",
       " 'take': 14743,\n",
       " 'our': 10493,\n",
       " 'home': 7283,\n",
       " 'totally': 15327,\n",
       " 'dissapointed': 4585,\n",
       " 'had': 6868,\n",
       " 'purchased': 11785,\n",
       " 'coupon': 3676,\n",
       " 'from': 6195,\n",
       " 'travelzoo': 15435,\n",
       " 'try': 15520,\n",
       " 'out': 10496,\n",
       " 'given': 6483,\n",
       " 'its': 7980,\n",
       " 'location': 8814,\n",
       " 'would': 16606,\n",
       " 'thought': 15104,\n",
       " 'it': 7971,\n",
       " 'was': 16252,\n",
       " 'going': 6551,\n",
       " 'be': 1522,\n",
       " 'very': 16032,\n",
       " 'upscale': 15861,\n",
       " 'were': 16362,\n",
       " 'expecting': 5440,\n",
       " 'whole': 16429,\n",
       " 'lot': 8881,\n",
       " 'service': 13193,\n",
       " 'great': 6693,\n",
       " 'but': 2312,\n",
       " 'not': 10148,\n",
       " 'worth': 16601,\n",
       " 'itself': 7981,\n",
       " 'outdated': 10500,\n",
       " '80': 284,\n",
       " 'crab': 3721,\n",
       " 'cakes': 2387,\n",
       " 'appertizer': 933,\n",
       " 'cold': 3192,\n",
       " 'when': 16389,\n",
       " 'served': 13188,\n",
       " 'told': 15254,\n",
       " 'waiter': 16181,\n",
       " 'who': 16425,\n",
       " 'turn': 15556,\n",
       " 'chef': 2828,\n",
       " 'message': 9416,\n",
       " 'passed': 10776,\n",
       " 'back': 1310,\n",
       " 'she': 13272,\n",
       " 'appologized': 957,\n",
       " 'ordred': 10434,\n",
       " '12oz': 52,\n",
       " 'new': 10013,\n",
       " 'york': 16724,\n",
       " 'strip': 14358,\n",
       " 'ala': 646,\n",
       " 'carte': 2573,\n",
       " '29': 153,\n",
       " '95': 307,\n",
       " 'oz': 10602,\n",
       " 'pure': 11789,\n",
       " 'fat': 5622,\n",
       " 'price': 11555,\n",
       " 'expect': 5435,\n",
       " 'meat': 9318,\n",
       " 'than': 15015,\n",
       " 'ordered': 10428,\n",
       " 'wild': 16461,\n",
       " 'mushroom': 9844,\n",
       " 'pizza': 11174,\n",
       " 'ok': 10322,\n",
       " 'needs': 9972,\n",
       " 'an': 797,\n",
       " 'over': 10532,\n",
       " 'haul': 7014,\n",
       " 'major': 9048,\n",
       " 'way': 16300,\n",
       " 'if': 7534,\n",
       " 'want': 16220,\n",
       " 'make': 9051,\n",
       " 'any': 879,\n",
       " 'money': 9689,\n",
       " 'at': 1157,\n",
       " 'saturday': 12905,\n",
       " 'night': 10052,\n",
       " '7pm': 282,\n",
       " 'empty': 5125,\n",
       " 'think': 15082,\n",
       " 'highlight': 7186,\n",
       " 'meal': 9303,\n",
       " 'bottle': 1981,\n",
       " 'cabinet': 2365,\n",
       " 'costco': 3635,\n",
       " 'travel': 15428,\n",
       " 'recently': 12073,\n",
       " 'returned': 12444,\n",
       " 'trip': 15483,\n",
       " 'big': 1704,\n",
       " 'island': 7958,\n",
       " 'hi': 7168,\n",
       " 'arranged': 1038,\n",
       " 'throught': 15130,\n",
       " 'shopping': 13361,\n",
       " 'around': 1033,\n",
       " 'found': 6083,\n",
       " 'their': 15039,\n",
       " 'prices': 11559,\n",
       " 'phone': 11049,\n",
       " 'able': 340,\n",
       " 'all': 679,\n",
       " 'arrangements': 1040,\n",
       " 'airfair': 621,\n",
       " 'condo': 3394,\n",
       " 'car': 2505,\n",
       " 'didn': 4411,\n",
       " 'with': 16526,\n",
       " 'bs': 2197,\n",
       " 'on': 10352,\n",
       " 'hilo': 7206,\n",
       " 'adjust': 485,\n",
       " 'dates': 4053,\n",
       " 'according': 398,\n",
       " 'plan': 11196,\n",
       " 'being': 1599,\n",
       " 'accurate': 405,\n",
       " 'outstanding': 10527,\n",
       " 'value': 15939,\n",
       " 'gas': 6356,\n",
       " 'did': 4409,\n",
       " 'some': 13798,\n",
       " 'souvenier': 13885,\n",
       " 'kona': 8363,\n",
       " 'again': 573,\n",
       " 'saved': 12923,\n",
       " 've': 15968,\n",
       " 'shopped': 13358,\n",
       " 'years': 16679,\n",
       " 'becoming': 1555,\n",
       " 'groupie': 6766,\n",
       " 'been': 1567,\n",
       " 'couple': 3672,\n",
       " 'there': 15057,\n",
       " 'advantages': 520,\n",
       " 'rarely': 11967,\n",
       " 'busy': 2311,\n",
       " 'larger': 8478,\n",
       " 'groups': 6770,\n",
       " 'last': 8490,\n",
       " 'few': 5720,\n",
       " 'times': 15190,\n",
       " 'experienced': 5448,\n",
       " 'average': 1253,\n",
       " 'lousy': 8897,\n",
       " 'joining': 8119,\n",
       " 'already': 719,\n",
       " 'seated': 13085,\n",
       " 'group': 6765,\n",
       " '28': 152,\n",
       " 'high': 7181,\n",
       " 'school': 12990,\n",
       " 'hostess': 7369,\n",
       " 'wagged': 16174,\n",
       " 'her': 7142,\n",
       " 'finger': 5800,\n",
       " 'face': 5528,\n",
       " 'like': 8695,\n",
       " 'east': 4943,\n",
       " 'german': 6428,\n",
       " 'border': 1953,\n",
       " 'guard': 6800,\n",
       " 'waffle': 16169,\n",
       " 'house': 7390,\n",
       " 'waitress': 16184,\n",
       " 'poked': 11295,\n",
       " 'shoulder': 13376,\n",
       " 'attention': 1197,\n",
       " 'wife': 16449,\n",
       " 'chicken': 2855,\n",
       " 'dish': 4540,\n",
       " 'sent': 13162,\n",
       " 'as': 1080,\n",
       " 'measly': 9314,\n",
       " 'pieces': 11099,\n",
       " 'beers': 1571,\n",
       " 'tap': 14792,\n",
       " 'garbage': 6337,\n",
       " 'muffler': 9815,\n",
       " 'shop': 13354,\n",
       " 'town': 15357,\n",
       " 'shops': 13362,\n",
       " 'trust': 15513,\n",
       " 'mighty': 9482,\n",
       " 'them': 15042,\n",
       " 'greg': 6714,\n",
       " 'has': 6999,\n",
       " 'worked': 16580,\n",
       " 'multiple': 9824,\n",
       " 'cars': 2571,\n",
       " 'done': 4687,\n",
       " 'job': 8105,\n",
       " 'ever': 5333,\n",
       " 'issues': 7968,\n",
       " 'he': 7041,\n",
       " 'takes': 14748,\n",
       " 'care': 2525,\n",
       " 'problem': 11605,\n",
       " 'no': 10084,\n",
       " 'questions': 11856,\n",
       " 'asked': 1102,\n",
       " 'just': 8189,\n",
       " 'start': 14159,\n",
       " 'off': 10287,\n",
       " 'by': 2346,\n",
       " 'saying': 12940,\n",
       " 'egg': 5023,\n",
       " 'salad': 12788,\n",
       " 'sandwiches': 12862,\n",
       " 'probably': 11603,\n",
       " 'tried': 15473,\n",
       " 'sandwich': 12860,\n",
       " 'anywhere': 892,\n",
       " 'serves': 13192,\n",
       " 'sacks': 12762,\n",
       " 'far': 5600,\n",
       " 'entitled': 5217,\n",
       " 'dali': 3991,\n",
       " 'life': 8673,\n",
       " 'live': 8775,\n",
       " 'north': 10136,\n",
       " 'will': 16470,\n",
       " 'literally': 8770,\n",
       " 'many': 9120,\n",
       " 'miles': 9493,\n",
       " 'eat': 4949,\n",
       " 'top': 15293,\n",
       " 'wonderful': 16554,\n",
       " 'menu': 9396,\n",
       " 'whenever': 16390,\n",
       " 'order': 10427,\n",
       " 'comes': 3237,\n",
       " 'little': 8773,\n",
       " 'cookie': 3558,\n",
       " 'cookies': 3559,\n",
       " 'can': 2435,\n",
       " 'purchase': 11784,\n",
       " 'dough': 4722,\n",
       " 'also': 721,\n",
       " 'other': 10485,\n",
       " 'delicious': 4220,\n",
       " 'dessert': 4342,\n",
       " 'bars': 1458,\n",
       " 'salads': 12789,\n",
       " 'sale': 12796,\n",
       " 'well': 16356,\n",
       " 'eating': 4958,\n",
       " 'least': 8566,\n",
       " '14': 56,\n",
       " 'hope': 7330,\n",
       " 'never': 10012,\n",
       " 'go': 6532,\n",
       " 'away': 1275,\n",
       " 'saw': 12934,\n",
       " 'triple': 15485,\n",
       " 'so': 13738,\n",
       " 'decided': 4131,\n",
       " 'expectations': 5438,\n",
       " 'arrived': 1048,\n",
       " 'shocked': 13339,\n",
       " 'thursday': 15145,\n",
       " 'morning': 9742,\n",
       " 'line': 8724,\n",
       " 'extra': 5494,\n",
       " 'long': 8842,\n",
       " 'wait': 16179,\n",
       " 'patiently': 10810,\n",
       " 'don': 4682,\n",
       " 'needless': 9971,\n",
       " 'say': 12938,\n",
       " 'because': 1548,\n",
       " 'quickly': 11864,\n",
       " 'pork': 11358,\n",
       " 'chop': 2938,\n",
       " 'eggs': 5030,\n",
       " 'hash': 7000,\n",
       " 'browns': 2168,\n",
       " 'fabulous': 5526,\n",
       " 'toast': 15229,\n",
       " 'made': 8998,\n",
       " 'grape': 6659,\n",
       " 'jelly': 8061,\n",
       " 'recommend': 12097,\n",
       " 'anyone': 885,\n",
       " 'downtown': 4743,\n",
       " 'area': 1004,\n",
       " 'sure': 14583,\n",
       " 'bacon': 1324,\n",
       " 'hit': 7232,\n",
       " 'wine': 16492,\n",
       " 'down': 4732,\n",
       " 'wednesday': 16325,\n",
       " 'happenin': 6955,\n",
       " 'tastings': 14837,\n",
       " 'courtesy': 3685,\n",
       " 'kyot': 8401,\n",
       " 'wrong': 16639,\n",
       " 'event': 5330,\n",
       " 'minutes': 9546,\n",
       " 'maybe': 9272,\n",
       " 'spot': 14035,\n",
       " 'case': 2583,\n",
       " 'hoppin': 7339,\n",
       " 'walked': 16197,\n",
       " 'looks': 8855,\n",
       " 'possibly': 11391,\n",
       " 'manager': 9088,\n",
       " 'owner': 10590,\n",
       " 'then': 15047,\n",
       " 'pointed': 11285,\n",
       " 'towards': 15350,\n",
       " 'room': 12641,\n",
       " 'where': 16391,\n",
       " 'held': 7114,\n",
       " 'about': 343,\n",
       " 'tables': 14711,\n",
       " 'people': 10927,\n",
       " 'talk': 14757,\n",
       " 'early': 4926,\n",
       " 'bird': 1735,\n",
       " 'gets': 6435,\n",
       " 'worm': 16591,\n",
       " 'finish': 5808,\n",
       " 'table': 14709,\n",
       " 'sit': 13510,\n",
       " 'patio': 10812,\n",
       " 'said': 12778,\n",
       " 'yes': 16698,\n",
       " 'right': 12522,\n",
       " 'help': 7124,\n",
       " 'meantime': 9312,\n",
       " 'another': 850,\n",
       " 'woman': 16548,\n",
       " 'friend': 6167,\n",
       " 'took': 15283,\n",
       " 'nearby': 9954,\n",
       " '20': 107,\n",
       " 'later': 8498,\n",
       " 'looking': 8853,\n",
       " 'came': 2417,\n",
       " 'helped': 7125,\n",
       " 'yet': 16701,\n",
       " 'left': 8583,\n",
       " 'return': 12443,\n",
       " 'small': 13638,\n",
       " 'happy': 6963,\n",
       " 'hour': 7388,\n",
       " 'flier': 5905,\n",
       " 'handed': 6914,\n",
       " 'distance': 4592,\n",
       " 'even': 5327,\n",
       " 'come': 3234,\n",
       " 'know': 8351,\n",
       " 're': 12013,\n",
       " 'wondering': 16557,\n",
       " 'why': 16440,\n",
       " '15': 60,\n",
       " 'wanted': 16222,\n",
       " 'see': 13113,\n",
       " 'how': 7404,\n",
       " 'bad': 1326,\n",
       " 'frankly': 6118,\n",
       " 'same': 12833,\n",
       " 'full': 6252,\n",
       " 'detailed': 4352,\n",
       " 'menus': 9398,\n",
       " 'deals': 4093,\n",
       " 'ready': 12028,\n",
       " 'um': 15635,\n",
       " 'gave': 6375,\n",
       " '10': 16,\n",
       " 'seconds': 13096,\n",
       " 'ago': 587,\n",
       " 'enjoy': 5179,\n",
       " 'look': 8851,\n",
       " 'app': 913,\n",
       " 'specialty': 13945,\n",
       " 'appetizer': 938,\n",
       " 'much': 9807,\n",
       " 'whatever': 16378,\n",
       " 'deal': 4087,\n",
       " 'drink': 4801,\n",
       " 'section': 13103,\n",
       " 'claim': 3029,\n",
       " 'glasses': 6497,\n",
       " 'excited': 5387,\n",
       " 'wines': 16493,\n",
       " 'reality': 12031,\n",
       " 'consisted': 3467,\n",
       " 'three': 15116,\n",
       " 'different': 4427,\n",
       " 'weren': 16363,\n",
       " 'women': 16549,\n",
       " 'outside': 10525,\n",
       " 'leave': 8569,\n",
       " 'having': 7026,\n",
       " 'enough': 5189,\n",
       " 'shenanigans': 13295,\n",
       " '30': 162,\n",
       " 'since': 13482,\n",
       " 'next': 10031,\n",
       " 'door': 4699,\n",
       " 'sprouts': 14064,\n",
       " 'apps': 978,\n",
       " 'own': 10588,\n",
       " 'bet': 1660,\n",
       " 'plenty': 11250,\n",
       " 'experiences': 5449,\n",
       " 'continue': 3514,\n",
       " 'visit': 16107,\n",
       " 'happily': 6961,\n",
       " 'experience': 5447,\n",
       " 'caused': 2628,\n",
       " 'write': 16632,\n",
       " 'usually': 15894,\n",
       " 'instance': 7807,\n",
       " 'wish': 16521,\n",
       " 'actually': 447,\n",
       " 'review': 12460,\n",
       " 'poor': 11337,\n",
       " 'substandard': 14446,\n",
       " 'giving': 6486,\n",
       " 'understand': 15696,\n",
       " 'isn': 7962,\n",
       " 'better': 1667,\n",
       " 'talented': 14753,\n",
       " 'others': 10486,\n",
       " 'however': 7407,\n",
       " 'business': 2300,\n",
       " 'should': 13375,\n",
       " 'customer': 3946,\n",
       " 'cliched': 3082,\n",
       " 'less': 8626,\n",
       " 'star': 14146,\n",
       " 'oh': 10310,\n",
       " 'yeah': 16674,\n",
       " 'slapped': 13575,\n",
       " 'yelped': 16689,\n",
       " 'card': 2516,\n",
       " 'pepperoni': 10937,\n",
       " 'amazed': 749,\n",
       " 'herb': 7143,\n",
       " 'vegetable': 15976,\n",
       " 'garden': 6340,\n",
       " 'front': 6197,\n",
       " 'ingredients': 7747,\n",
       " 'these': 15065,\n",
       " 'days': 4067,\n",
       " 'dishes': 4541,\n",
       " 'cheese': 2818,\n",
       " 'platters': 11225,\n",
       " 'mouth': 9782,\n",
       " 'watering': 16285,\n",
       " 'speak': 13926,\n",
       " 'day': 4064,\n",
       " 'll': 8788,\n",
       " 'keep': 8244,\n",
       " 'adding': 469,\n",
       " 'stay': 14186,\n",
       " 'tuned': 15547,\n",
       " 'btw': 2198,\n",
       " 'decor': 4149,\n",
       " 'reminds': 12272,\n",
       " 'vig': 16066,\n",
       " 'definitely': 4188,\n",
       " 'spacious': 13898,\n",
       " 'dining': 4462,\n",
       " 'sushi': 14612,\n",
       " 'time': 15184,\n",
       " 'garlic': 6346,\n",
       " 'knots': 8350,\n",
       " 'favorite': 5643,\n",
       " 'alot': 716,\n",
       " 'panda': 10687,\n",
       " 'family': 5581,\n",
       " 'run': 12728,\n",
       " 'chinese': 2894,\n",
       " 'most': 9757,\n",
       " 'importantly': 7596,\n",
       " 'style': 14421,\n",
       " 'meals': 9304,\n",
       " 'personal': 10987,\n",
       " 'empress': 5122,\n",
       " 'sweeter': 14655,\n",
       " 'orange': 10415,\n",
       " 'flavor': 5884,\n",
       " 'melts': 9371,\n",
       " 'your': 16732,\n",
       " 'kung': 8396,\n",
       " 'pao': 10705,\n",
       " 'two': 15598,\n",
       " 'shrimp': 13403,\n",
       " 'leftovers': 8585,\n",
       " 'thanks': 15024,\n",
       " 'generous': 6407,\n",
       " 'portions': 11369,\n",
       " 'selection': 13131,\n",
       " 'sake': 12784,\n",
       " 'beer': 1570,\n",
       " 'everyone': 5340,\n",
       " 'else': 5078,\n",
       " 'checks': 2809,\n",
       " 'man': 9083,\n",
       " 'leaving': 8571,\n",
       " 'six': 13519,\n",
       " 'half': 6889,\n",
       " 'search': 13073,\n",
       " 'gotten': 6610,\n",
       " 'point': 11283,\n",
       " 'settled': 13205,\n",
       " 'decent': 4125,\n",
       " 'work': 16579,\n",
       " 'colleague': 3202,\n",
       " 'suggested': 14496,\n",
       " 'dine': 4454,\n",
       " 'corporate': 3610,\n",
       " 'meeting': 9348,\n",
       " 'while': 16398,\n",
       " 'realized': 12033,\n",
       " 'native': 9933,\n",
       " 'metro': 9435,\n",
       " 'thing': 15077,\n",
       " 'trusted': 15514,\n",
       " 'his': 7227,\n",
       " 'palate': 10656,\n",
       " 'sense': 13156,\n",
       " 'pleased': 11243,\n",
       " 'diners': 4458,\n",
       " 'predominately': 11467,\n",
       " 'asian': 1098,\n",
       " 'knew': 8337,\n",
       " 'market': 9165,\n",
       " 'things': 15079,\n",
       " 'signaled': 13445,\n",
       " 'thrilled': 15123,\n",
       " 'familiar': 5579,\n",
       " 'dim': 4448,\n",
       " 'sum': 14510,\n",
       " 'carts': 2578,\n",
       " 'rolling': 12624,\n",
       " 'through': 15128,\n",
       " 'party': 10768,\n",
       " 'shared': 13254,\n",
       " 'fantastic': 5596,\n",
       " 'succulent': 14469,\n",
       " 'orders': 10431,\n",
       " 'equally': 5244,\n",
       " 'veggies': 15981,\n",
       " 'brightly': 2120,\n",
       " 'colored': 3215,\n",
       " 'tastes': 14830,\n",
       " 'distinctive': 4598,\n",
       " 'hot': 7374,\n",
       " 'sour': 13871,\n",
       " 'soup': 13868,\n",
       " 'perfect': 10949,\n",
       " 'filled': 5766,\n",
       " 'kinds': 8310,\n",
       " 'mushrooms': 9845,\n",
       " 'etc': 5301,\n",
       " 'delighted': 4226,\n",
       " 'reminisced': 12273,\n",
       " 'dinners': 4466,\n",
       " 'chinatown': 2893,\n",
       " 'ny': 10224,\n",
       " 'comparable': 3290,\n",
       " 'close': 3108,\n",
       " 'regular': 12197,\n",
       " 'chandler': 2741,\n",
       " 'clear': 3067,\n",
       " 'side': 13428,\n",
       " 'world': 16588,\n",
       " 'figure': 5750,\n",
       " 'marco': 9133,\n",
       " 'polo': 11314,\n",
       " 'palace': 10653,\n",
       " 'prompt': 11671,\n",
       " 'rounded': 12678,\n",
       " 'hooray': 7326,\n",
       " 'stayed': 14189,\n",
       " 'hotel': 7377,\n",
       " 'tuscany': 15566,\n",
       " 'scallops': 12951,\n",
       " 'pasta': 10788,\n",
       " 'excellent': 5374,\n",
       " 'server': 13189,\n",
       " 'bliss': 1820,\n",
       " 'haven': 7024,\n",
       " 'quality': 11833,\n",
       " 'dinner': 4465,\n",
       " 'stupendous': 14413,\n",
       " 'every': 5336,\n",
       " 'cent': 2674,\n",
       " 'must': 9856,\n",
       " 'yin': 16705,\n",
       " 'yang': 16663,\n",
       " 'martini': 9195,\n",
       " 'creative': 3779,\n",
       " 'presented': 11514,\n",
       " 'smiles': 13664,\n",
       " 'smores': 13684,\n",
       " 'desert': 4315,\n",
       " 'hands': 6928,\n",
       " 'connosiuer': 3444,\n",
       " 'critical': 3821,\n",
       " 'college': 3209,\n",
       " 'kids': 8288,\n",
       " 'ra': 11887,\n",
       " 'mediocre': 9341,\n",
       " 'list': 8755,\n",
       " 'irk': 7938,\n",
       " 'during': 4891,\n",
       " 'co': 3145,\n",
       " 'workers': 16582,\n",
       " 'packed': 10616,\n",
       " 'extremely': 5503,\n",
       " 'meet': 9347,\n",
       " 'coworkers': 3708,\n",
       " 'tired': 15214,\n",
       " 'chill': 2878,\n",
       " 'past': 10787,\n",
       " 'friends': 6172,\n",
       " 'alone': 710,\n",
       " 'quite': 11877,\n",
       " 'bit': 1749,\n",
       " 'servings': 13197,\n",
       " 'obviously': 10256,\n",
       " 'low': 8911,\n",
       " 'zen': 16785,\n",
       " '32': 172,\n",
       " 'choice': 2922,\n",
       " 'sakebomber': 12785,\n",
       " 'loud': 8886,\n",
       " 'noises': 10098,\n",
       " 'trying': 15521,\n",
       " 'louder': 8887,\n",
       " 'music': 9848,\n",
       " 'person': 10985,\n",
       " 'overall': 10533,\n",
       " 'tip': 15207,\n",
       " 'priced': 11556,\n",
       " 'works': 16587,\n",
       " 'horrible': 7351,\n",
       " 'twice': 15586,\n",
       " 'gone': 6564,\n",
       " 'messing': 9420,\n",
       " 'starbucks': 14148,\n",
       " 'hard': 6969,\n",
       " 'believe': 1604,\n",
       " 'attitude': 1202,\n",
       " 'lady': 8427,\n",
       " 'doesn': 4648,\n",
       " 'ordering': 10429,\n",
       " 'someone': 13803,\n",
       " 'fix': 5846,\n",
       " 'known': 8356,\n",
       " 'ya': 16655,\n",
       " 'employees': 5117,\n",
       " 'rude': 12708,\n",
       " 'makes': 9055,\n",
       " 'anymore': 884,\n",
       " 'stopped': 14290,\n",
       " 'still': 14252,\n",
       " 'mom': 9679,\n",
       " 'goes': 6549,\n",
       " 'coffee': 3182,\n",
       " 'coast': 3151,\n",
       " 'deli': 4210,\n",
       " 'west': 16365,\n",
       " 'appalachians': 914,\n",
       " 'creams': 3771,\n",
       " 'pricy': 11566,\n",
       " 'apparently': 921,\n",
       " 'free': 6134,\n",
       " 'summer': 14515,\n",
       " 'kept': 8256,\n",
       " 'stars': 14158,\n",
       " 'frown': 6207,\n",
       " 'birthday': 1739,\n",
       " 'spirits': 13998,\n",
       " 'written': 16637,\n",
       " 'clarity': 3042,\n",
       " 'margaritas': 9140,\n",
       " 'strong': 14376,\n",
       " 'check': 2797,\n",
       " '18': 75,\n",
       " 'mahi': 9025,\n",
       " 'each': 4917,\n",
       " '24': 135,\n",
       " 'rancid': 11948,\n",
       " 'yup': 16765,\n",
       " 'fajitas': 5564,\n",
       " 'jumbo': 8173,\n",
       " 'charge': 2763,\n",
       " 'entree': 5222,\n",
       " 'pay': 10844,\n",
       " 'onions': 10360,\n",
       " 'once': 10353,\n",
       " 'week': 16329,\n",
       " 'asada': 1081,\n",
       " 'enchilada': 5138,\n",
       " 'avocado': 1260,\n",
       " 'lover': 8905,\n",
       " 'ripest': 12542,\n",
       " 'avocados': 1261,\n",
       " 'f724': 5521,\n",
       " 'bother': 1975,\n",
       " 'asking': 1104,\n",
       " 'entering': 5201,\n",
       " 'reply': 12331,\n",
       " 'spoke': 14020,\n",
       " 'acknowledged': 419,\n",
       " 'presence': 11511,\n",
       " 'ticket': 15153,\n",
       " 'happened': 6954,\n",
       " 'effort': 5020,\n",
       " 'tell': 14925,\n",
       " 'scathing': 12969,\n",
       " 'deplorable': 4288,\n",
       " 'sadly': 12768,\n",
       " 'dmv': 4628,\n",
       " 'modeled': 9641,\n",
       " 'agonizing': 588,\n",
       " 'depths': 4297,\n",
       " 'hell': 7117,\n",
       " 'musac': 9837,\n",
       " 'score': 13009,\n",
       " 'playing': 11233,\n",
       " 'voracious': 16143,\n",
       " 'stench': 14221,\n",
       " 'air': 619,\n",
       " 'spastic': 13921,\n",
       " 'children': 2869,\n",
       " 'running': 12731,\n",
       " 'version': 16028,\n",
       " 'politicians': 11310,\n",
       " 'campaigning': 2426,\n",
       " 'late': 8496,\n",
       " 'complete': 3331,\n",
       " 'emissions': 5104,\n",
       " 'test': 14990,\n",
       " 'those': 15101,\n",
       " 'dmvs': 4629,\n",
       " 'open': 10379,\n",
       " 'saturdays': 12906,\n",
       " 'map': 9121,\n",
       " 'precious': 11458,\n",
       " 'fortunate': 6071,\n",
       " 'near': 9953,\n",
       " 'seriously': 13180,\n",
       " 'entire': 5215,\n",
       " 'valley': 15936,\n",
       " 'receiving': 12071,\n",
       " 'such': 14471,\n",
       " 'magnificent': 9018,\n",
       " 'welcome': 16352,\n",
       " 'among': 781,\n",
       " 'painful': 10633,\n",
       " 'simplify': 13476,\n",
       " 'let': 8630,\n",
       " 'feel': 5678,\n",
       " 'constitutes': 3479,\n",
       " 'acceptable': 371,\n",
       " 'behavior': 1594,\n",
       " 'public': 11732,\n",
       " 'bathe': 1492,\n",
       " 'combination': 3227,\n",
       " 'water': 16280,\n",
       " 'soap': 13742,\n",
       " 'along': 711,\n",
       " 'massaging': 9221,\n",
       " 'skin': 13553,\n",
       " 'creates': 3775,\n",
       " 'outcome': 10499,\n",
       " 'pleasing': 11244,\n",
       " 'scrub': 13050,\n",
       " 'behind': 1595,\n",
       " 'ears': 4932,\n",
       " 'reference': 12142,\n",
       " 'website': 16318,\n",
       " 'notorious': 10164,\n",
       " 'http': 7416,\n",
       " 'www': 16645,\n",
       " 'craigslist': 3740,\n",
       " 'org': 10443,\n",
       " 'htf': 7414,\n",
       " '755891987': 278,\n",
       " 'html': 7415,\n",
       " 'waiting': 16183,\n",
       " 'tapping': 14800,\n",
       " 'foot': 6022,\n",
       " 'rocking': 12609,\n",
       " 'forth': 6070,\n",
       " 'chair': 2712,\n",
       " 'bouncing': 1999,\n",
       " 'leg': 8586,\n",
       " 'aren': 1006,\n",
       " 'faster': 5620,\n",
       " 'watching': 16279,\n",
       " 'clock': 3102,\n",
       " 'anxious': 877,\n",
       " 'leash': 8563,\n",
       " 'cannot': 2465,\n",
       " 'quietly': 11868,\n",
       " 'winter': 16506,\n",
       " 'fine': 5794,\n",
       " 'cute': 3953,\n",
       " 'monster': 9702,\n",
       " 'aisles': 634,\n",
       " 'honey': 7307,\n",
       " 'bun': 2261,\n",
       " 'squeezing': 14083,\n",
       " 'between': 1671,\n",
       " 'fingers': 5805,\n",
       " 'fake': 5566,\n",
       " 'falls': 5575,\n",
       " 'five': 5845,\n",
       " 'row': 12686,\n",
       " 'maintain': 9039,\n",
       " 'quiet': 11866,\n",
       " 'tones': 15272,\n",
       " 'ensure': 5197,\n",
       " 'appropriate': 973,\n",
       " 'conversations': 3541,\n",
       " 'taking': 14749,\n",
       " 'making': 9060,\n",
       " 'calls': 2404,\n",
       " 'sorry': 13851,\n",
       " 'stripping': 14366,\n",
       " 'popped': 11342,\n",
       " 'into': 7877,\n",
       " 'mind': 9515,\n",
       " 'failed': 5550,\n",
       " 'audition': 1218,\n",
       " 'le': 8539,\n",
       " 'girl': 6470,\n",
       " 'continued': 3515,\n",
       " 'conversation': 3539,\n",
       " 'woodworker': 16571,\n",
       " 'acknowledge': 418,\n",
       " 'humans': 7440,\n",
       " 'fact': 5540,\n",
       " 'sideways': 13435,\n",
       " 'disappear': 4490,\n",
       " 'quit': 11876,\n",
       " 'bumping': 2259,\n",
       " 'hitting': 7235,\n",
       " 'unless': 15770,\n",
       " 'flirting': 5917,\n",
       " 'need': 9966,\n",
       " 'level': 8636,\n",
       " 'brilliant': 2122,\n",
       " 'recipe': 12083,\n",
       " 'dashing': 4050,\n",
       " 'dreams': 4778,\n",
       " 'gutting': 6850,\n",
       " 'humanity': 7439,\n",
       " 'manage': 9084,\n",
       " 'painfully': 10634,\n",
       " 'lit': 8766,\n",
       " 'reeks': 12137,\n",
       " 'brother': 2158,\n",
       " 'old': 10328,\n",
       " 'gym': 6853,\n",
       " 'shoes': 13345,\n",
       " 'dad': 3979,\n",
       " 'dirty': 4486,\n",
       " 'underwear': 15703,\n",
       " 'award': 1271,\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the vocabulary that was generated, containing 16,825 unique words.\n",
    "#   'vocabulary_' is a dictionary that converts each word to its index in the sparse matrix.\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show vectorizer options.\n",
    "# Don't convert to lowercase.\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, CountVectorizer converts all text to lowercase before generating features. Otherwise, \"Pizza\" at the start of a sentence becomes a different feature from \"pizza\" in the middle of a sentence.\n",
    "\n",
    "On the other hand, you would want different features corresponding to \"Apple\" the company and \"apple\" the fruit, so this step does discard some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Don't convert to lowercase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** What is the vocabulary size for CountVectorizer on this dataset with `lowercase=False`? Is this size greater or smaller than the size with `lowercase=True`? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create document-term matrices using default options for CountVectorizer.\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use Naive Bayes to predict the star rating.\n",
    "# Use Naive Bayes to predict the star rating.\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91878669275929548"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate accuracy.\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    838\n",
       "1    184\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check label balance\n",
    "# Check label balance\n",
    "# with accuracy, you have to check the balance between the classes\n",
    "# 838/184\n",
    "\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy guessing 5 every time: 0.819960861057\n"
     ]
    }
   ],
   "source": [
    "#predict 5 everytime, and record 1 if right, 0 if wrong\n",
    "#take the mean, gives accuracy of 82%, accuracy returns better 92%, so there's an improvement\n",
    "\n",
    "\n",
    "print('Accuracy guessing 5 every time:', np.where(y_test==5, 1, 0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81996086105675148"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternatively\n",
    "(y_test == 5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model predicted ~92% accuracy, which is an improvement over this baseline 82% accuracy (assuming our model always predicts 5 stars).\n",
    "\n",
    "Let's look more into how the vectorizer works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that accepts a vectorizer and calculates the accuracy.\n",
    "\n",
    "def tokenize_test(vect):\n",
    "    vect.fit(X_train)\n",
    "    X_train_dtm = vect.transform(X_train)\n",
    "    print('Number of features: ', X_train_dtm.shape[1])\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    \n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(X_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  16825\n",
      "Accuracy:  0.918786692759\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  8783\n",
      "Accuracy:  0.924657534247\n"
     ]
    }
   ],
   "source": [
    "# min_df=2 says to ignore words that occur less than twice ('df' means \"document frequency\").\n",
    "# if word occurs in less than 2 documents, ignore it\n",
    "# Cut the # of features for 16825 down to 8783\n",
    "# min_df = minimum document frequency\n",
    "vect = CountVectorizer(min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  5000\n",
      "Accuracy:  0.918786692759\n"
     ]
    }
   ],
   "source": [
    "#max features defines the max number words in our corpus\n",
    "#take the 5000 most common words with a min freq of 2\n",
    "\n",
    "vect = CountVectorizer(min_df=2, max_features=5000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CountVectorizer in module sklearn.feature_extraction.text:\n",
      "\n",
      "class CountVectorizer(sklearn.base.BaseEstimator, VectorizerMixin)\n",
      " |  Convert a collection of text documents to a matrix of token counts\n",
      " |  \n",
      " |  This implementation produces a sparse representation of the counts using\n",
      " |  scipy.sparse.csr_matrix.\n",
      " |  \n",
      " |  If you do not provide an a-priori dictionary and you do not use an analyzer\n",
      " |  that does some kind of feature selection then the number of features will\n",
      " |  be equal to the vocabulary size found by analyzing the data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  input : string {'filename', 'file', 'content'}\n",
      " |      If 'filename', the sequence passed as an argument to fit is\n",
      " |      expected to be a list of filenames that need reading to fetch\n",
      " |      the raw content to analyze.\n",
      " |  \n",
      " |      If 'file', the sequence items must have a 'read' method (file-like\n",
      " |      object) that is called to fetch the bytes in memory.\n",
      " |  \n",
      " |      Otherwise the input is expected to be the sequence strings or\n",
      " |      bytes items are expected to be analyzed directly.\n",
      " |  \n",
      " |  encoding : string, 'utf-8' by default.\n",
      " |      If bytes or files are given to analyze, this encoding is used to\n",
      " |      decode.\n",
      " |  \n",
      " |  decode_error : {'strict', 'ignore', 'replace'}\n",
      " |      Instruction on what to do if a byte sequence is given to analyze that\n",
      " |      contains characters not of the given `encoding`. By default, it is\n",
      " |      'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
      " |      values are 'ignore' and 'replace'.\n",
      " |  \n",
      " |  strip_accents : {'ascii', 'unicode', None}\n",
      " |      Remove accents during the preprocessing step.\n",
      " |      'ascii' is a fast method that only works on characters that have\n",
      " |      an direct ASCII mapping.\n",
      " |      'unicode' is a slightly slower method that works on any characters.\n",
      " |      None (default) does nothing.\n",
      " |  \n",
      " |  analyzer : string, {'word', 'char', 'char_wb'} or callable\n",
      " |      Whether the feature should be made of word or character n-grams.\n",
      " |      Option 'char_wb' creates character n-grams only from text inside\n",
      " |      word boundaries; n-grams at the edges of words are padded with space.\n",
      " |  \n",
      " |      If a callable is passed it is used to extract the sequence of features\n",
      " |      out of the raw, unprocessed input.\n",
      " |  \n",
      " |  preprocessor : callable or None (default)\n",
      " |      Override the preprocessing (string transformation) stage while\n",
      " |      preserving the tokenizing and n-grams generation steps.\n",
      " |  \n",
      " |  tokenizer : callable or None (default)\n",
      " |      Override the string tokenization step while preserving the\n",
      " |      preprocessing and n-grams generation steps.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |  ngram_range : tuple (min_n, max_n)\n",
      " |      The lower and upper boundary of the range of n-values for different\n",
      " |      n-grams to be extracted. All values of n such that min_n <= n <= max_n\n",
      " |      will be used.\n",
      " |  \n",
      " |  stop_words : string {'english'}, list, or None (default)\n",
      " |      If 'english', a built-in stop word list for English is used.\n",
      " |  \n",
      " |      If a list, that list is assumed to contain stop words, all of which\n",
      " |      will be removed from the resulting tokens.\n",
      " |      Only applies if ``analyzer == 'word'``.\n",
      " |  \n",
      " |      If None, no stop words will be used. max_df can be set to a value\n",
      " |      in the range [0.7, 1.0) to automatically detect and filter stop\n",
      " |      words based on intra corpus document frequency of terms.\n",
      " |  \n",
      " |  lowercase : boolean, True by default\n",
      " |      Convert all characters to lowercase before tokenizing.\n",
      " |  \n",
      " |  token_pattern : string\n",
      " |      Regular expression denoting what constitutes a \"token\", only used\n",
      " |      if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
      " |      or more alphanumeric characters (punctuation is completely ignored\n",
      " |      and always treated as a token separator).\n",
      " |  \n",
      " |  max_df : float in range [0.0, 1.0] or int, default=1.0\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly higher than the given threshold (corpus-specific\n",
      " |      stop words).\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  min_df : float in range [0.0, 1.0] or int, default=1\n",
      " |      When building the vocabulary ignore terms that have a document\n",
      " |      frequency strictly lower than the given threshold. This value is also\n",
      " |      called cut-off in the literature.\n",
      " |      If float, the parameter represents a proportion of documents, integer\n",
      " |      absolute counts.\n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  max_features : int or None, default=None\n",
      " |      If not None, build a vocabulary that only consider the top\n",
      " |      max_features ordered by term frequency across the corpus.\n",
      " |  \n",
      " |      This parameter is ignored if vocabulary is not None.\n",
      " |  \n",
      " |  vocabulary : Mapping or iterable, optional\n",
      " |      Either a Mapping (e.g., a dict) where keys are terms and values are\n",
      " |      indices in the feature matrix, or an iterable over terms. If not\n",
      " |      given, a vocabulary is determined from the input documents. Indices\n",
      " |      in the mapping should not be repeated and should not have any gap\n",
      " |      between 0 and the largest index.\n",
      " |  \n",
      " |  binary : boolean, default=False\n",
      " |      If True, all non zero counts are set to 1. This is useful for discrete\n",
      " |      probabilistic models that model binary events rather than integer\n",
      " |      counts.\n",
      " |  \n",
      " |  dtype : type, optional\n",
      " |      Type of the matrix returned by fit_transform() or transform().\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  vocabulary_ : dict\n",
      " |      A mapping of terms to feature indices.\n",
      " |  \n",
      " |  stop_words_ : set\n",
      " |      Terms that were ignored because they either:\n",
      " |  \n",
      " |        - occurred in too many documents (`max_df`)\n",
      " |        - occurred in too few documents (`min_df`)\n",
      " |        - were cut off by feature selection (`max_features`).\n",
      " |  \n",
      " |      This is only available if no vocabulary was given.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  HashingVectorizer, TfidfVectorizer\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The ``stop_words_`` attribute can get large and increase the model size\n",
      " |  when pickling. This attribute is provided only for introspection and can\n",
      " |  be safely removed using delattr or set to None before pickling.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      CountVectorizer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      VectorizerMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class 'numpy.int64'>)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, raw_documents, y=None)\n",
      " |      Learn a vocabulary dictionary of all tokens in the raw documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  fit_transform(self, raw_documents, y=None)\n",
      " |      Learn the vocabulary dictionary and return term-document matrix.\n",
      " |      \n",
      " |      This is equivalent to fit followed by transform, but more efficiently\n",
      " |      implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : array, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Array mapping from feature integer indices to feature name\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Return terms per document with nonzero entries in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array, sparse matrix}, shape = [n_samples, n_features]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_inv : list of arrays, len = n_samples\n",
      " |          List of arrays of terms.\n",
      " |  \n",
      " |  transform(self, raw_documents)\n",
      " |      Transform documents to document-term matrix.\n",
      " |      \n",
      " |      Extract token counts out of raw text documents using the vocabulary\n",
      " |      fitted with fit or the one provided to the constructor.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      raw_documents : iterable\n",
      " |          An iterable which yields either str, unicode or file objects.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : sparse matrix, [n_samples, n_features]\n",
      " |          Document-term matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from VectorizerMixin:\n",
      " |  \n",
      " |  build_analyzer(self)\n",
      " |      Return a callable that handles preprocessing and tokenization\n",
      " |  \n",
      " |  build_preprocessor(self)\n",
      " |      Return a function to preprocess the text before tokenization\n",
      " |  \n",
      " |  build_tokenizer(self)\n",
      " |      Return a function that splits a string into a sequence of tokens\n",
      " |  \n",
      " |  decode(self, doc)\n",
      " |      Decode the input into a string of unicode symbols\n",
      " |      \n",
      " |      The decoding strategy depends on the vectorizer parameters.\n",
      " |  \n",
      " |  get_stop_words(self)\n",
      " |      Build or fetch the effective stop words list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CountVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look next at other ways of preprocessing text!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ngrams'></a>\n",
    "### N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-grams are features which consist of N consecutive words. This is useful because using the bag-of-words model, treating `data scientist` as a single feature has more meaning than having two independent features `data` and `scientist`!\n",
    "\n",
    "Example:\n",
    "```\n",
    "my cat is awesome\n",
    "Unigrams (1-grams): 'my', 'cat', 'is', 'awesome'\n",
    "Bigrams (2-grams): 'my cat', 'cat is', 'is awesome'\n",
    "Trigrams (3-grams): 'my cat is', 'cat is awesome'\n",
    "4-grams: 'my cat is awesome'\n",
    "```\n",
    "\n",
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  16000\n",
      "Accuracy:  0.916829745597\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams.\n",
    "# we added more noise, model overfit\n",
    "# accuracy went down\n",
    "\n",
    "vect = CountVectorizer(ngram_range=(1,2), max_features=16000)\n",
    "vect.fit(X_train)\n",
    "X_train_dtm = vect.transform(X_train)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $n$ gets larger, the number of *unique* n-grams increases greatly, adding pure noise to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your face',\n",
       " 'your favorite',\n",
       " 'your first',\n",
       " 'your food',\n",
       " 'your friends',\n",
       " 'your guests',\n",
       " 'your hair',\n",
       " 'your hand',\n",
       " 'your hands',\n",
       " 'your job',\n",
       " 'your kids',\n",
       " 'your life',\n",
       " 'your looking',\n",
       " 'your meal',\n",
       " 'your mind',\n",
       " 'your money',\n",
       " 'your mouth',\n",
       " 'your name',\n",
       " 'your order',\n",
       " 'your own',\n",
       " 'your place',\n",
       " 'your questions',\n",
       " 'your restaurant',\n",
       " 'your server',\n",
       " 'your table',\n",
       " 'your taste',\n",
       " 'your time',\n",
       " 'your tongue',\n",
       " 'your typical',\n",
       " 'your way',\n",
       " 'yourself',\n",
       " 'yourself and',\n",
       " 'yourself favor',\n",
       " 'yourself with',\n",
       " 'yr',\n",
       " 'yr old',\n",
       " 'yuck',\n",
       " 'yum',\n",
       " 'yum yum',\n",
       " 'yummm',\n",
       " 'yummy',\n",
       " 'yummy and',\n",
       " 'yup',\n",
       " 'zach',\n",
       " 'zen',\n",
       " 'zero',\n",
       " 'zero stars',\n",
       " 'zinburger',\n",
       " 'zoo',\n",
       " 'zucchini']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Last 50 features\n",
    "vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopwords'></a>\n",
    "\n",
    "### Stop-Word Removal\n",
    "\n",
    "- **What:** This process is used to remove common words that will likely appear in any text.\n",
    "- **Why:** Because common words exist in most documents, they likely only add noise to your model and should be removed.\n",
    "\n",
    "**What are stop words?**\n",
    "Stop words are some of the most common words in a language. They are used so that a sentence makes sense grammatically, such as prepositions and determiners, e.g., \"to,\" \"the,\" \"and.\" However, they are so commonly used that they are generally worthless for predicting the class of a document. Since \"a\" appears in spam and non-spam emails, for example, it would only contribute noise to our model.\n",
    "\n",
    "Example: \n",
    "\n",
    "> 1. Original sentence: \"The dog jumped over the fence\"  \n",
    "> 2. After stop-word removal: \"dog jumped over fence\"\n",
    "\n",
    "The fact that there is a fence and a dog jumped over it can be derived with or without stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=16000, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show vectorizer options.\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {`english`}, list, or None (default)\n",
    "- If `english`, a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "- If None, no stop words will be used. `max_df` can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. (If `max_df` = 0.7, then if > 70% of documents contain a word it will not be included in the feature set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer stop words for English\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "#show the stop words\n",
    "vect.get_stop_words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  16528\n",
      "Accuracy:  0.915851272016\n"
     ]
    }
   ],
   "source": [
    "# removing stop words can only help\n",
    "# in this case only sort of improved the model\n",
    "\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cvec_opt'></a>\n",
    "### Other CountVectorizer Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `max_features`: int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top `max_features` ordered by term frequency across the corpus. This allows us to keep more common n-grams and remove ones that may appear once. If we include words that only occur once, this can lead to said features being highly associated with a class and cause overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  100\n",
      "Accuracy:  0.869863013699\n"
     ]
    }
   ],
   "source": [
    "# Remove English stop words and only keep 100 features.\n",
    "#86% but with less features\n",
    "vect = CountVectorizer(stop_words='english', max_features=100)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'area',\n",
       " 'atmosphere',\n",
       " 'awesome',\n",
       " 'bad',\n",
       " 'bar',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'came',\n",
       " 'cheese',\n",
       " 'chicken',\n",
       " 'clean',\n",
       " 'coffee',\n",
       " 'come',\n",
       " 'day',\n",
       " 'definitely',\n",
       " 'delicious',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'dinner',\n",
       " 'don',\n",
       " 'eat',\n",
       " 'excellent',\n",
       " 'experience',\n",
       " 'favorite',\n",
       " 'feel',\n",
       " 'food',\n",
       " 'free',\n",
       " 'fresh',\n",
       " 'friendly',\n",
       " 'friends',\n",
       " 'going',\n",
       " 'good',\n",
       " 'got',\n",
       " 'great',\n",
       " 'happy',\n",
       " 'home',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'just',\n",
       " 'know',\n",
       " 'like',\n",
       " 'little',\n",
       " 'll',\n",
       " 'location',\n",
       " 'long',\n",
       " 'looking',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'lunch',\n",
       " 'make',\n",
       " 'meal',\n",
       " 'menu',\n",
       " 'minutes',\n",
       " 'need',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'people',\n",
       " 'perfect',\n",
       " 'phoenix',\n",
       " 'pizza',\n",
       " 'place',\n",
       " 'pretty',\n",
       " 'prices',\n",
       " 'really',\n",
       " 'recommend',\n",
       " 'restaurant',\n",
       " 'right',\n",
       " 'said',\n",
       " 'salad',\n",
       " 'sandwich',\n",
       " 'sauce',\n",
       " 'say',\n",
       " 'service',\n",
       " 'staff',\n",
       " 'store',\n",
       " 'sure',\n",
       " 'table',\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'time',\n",
       " 'times',\n",
       " 'took',\n",
       " 'town',\n",
       " 'tried',\n",
       " 'try',\n",
       " 've',\n",
       " 'wait',\n",
       " 'want',\n",
       " 'way',\n",
       " 'went',\n",
       " 'wine',\n",
       " 'work',\n",
       " 'worth',\n",
       " 'years']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All 100 features\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with all other models, more features does not mean a better model. So, we must tune our feature generator to remove features whose predictive capability is none or very low.\n",
    "\n",
    "In this case, there is roughly a 1.6% increase in accuracy when we double the n-gram size and increase our max features by 1,000-fold. Note that if we restrict it to only unigrams, then the accuracy increases even more! So, bigrams were very likely adding more noise than signal. \n",
    "\n",
    "In the end, by only using 16,000 unigram features we came away with a much smaller, simpler, and easier-to-think-about model which also resulted in higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  10000\n",
      "Accuracy:  0.912915851272\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams, and limit the number of features.\n",
    "vect = CountVectorizer(ngram_range=(1,2), max_features=10000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  10000\n",
      "Accuracy:  0.920743639922\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,1), max_features=10000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:  43957\n",
      "Accuracy:  0.932485322896\n"
     ]
    }
   ],
   "source": [
    "# Include 1-grams and 2-grams, and only include terms that appear at least two times.\n",
    "vect = CountVectorizer(ngram_range=(1,2), min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** How does each of the following changes to the feature representation used affect the bias and variance of a resulting model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increasing min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "makes it more general\n",
    "increase bias, decrease variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using both unigrams and bigrams instead of just unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decrease bias, increase variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "increase bias, decrease variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decreasing `max_features`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase bias, decrease variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='textblob'></a>\n",
    "## Introduction to TextBlob\n",
    "\n",
    "You should already have downloaded TextBlob, a Python library used to explore common NLP tasks. If you havenâ€™t, please return to [this step](#textblob_install) for instructions on how to do so. Weâ€™ll be using this to organize our corpus for analysis.\n",
    "\n",
    "As mentioned earlier, you can read more on the [TextBlob website](https://textblob.readthedocs.io/en/dev/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\n\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I\\'ve ever had.  I\\'m pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\\n\\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I\\'ve ever had.\\n\\nAnyway, I can\\'t wait to go back!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first review.\n",
    "yelp_best_worst.loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-c11ec576b024>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-c11ec576b024>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install textblob\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install textblob\n",
    "python -m textblob.download_corpora lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it as a TextBlob object.\n",
    "# Save it as a TextBlob object.\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "review = TextBlob(yelp_best_worst.loc[0, 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the words.\n",
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the sentences.\n",
    "review.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some string methods are available.\n",
    "review.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stem'></a>\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "Stemming is a crude process of removing common endings from sentences, such as \"s\", \"es\", \"ly\", \"ing\", and \"ed\".\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form.\n",
    "- **Why:** This intelligently reduces the number of features by grouping together (hopefully) related words.\n",
    "- **Notes:**\n",
    "    - Stemming uses a simple and fast rule-based approach.\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing).\n",
    "    - Some search engines treat words with the same stem as synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize stemmer.\n",
    "stemmer = SnowballStemmer('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem each word.\n",
    "# lops off end of words\n",
    "[stemmer.stem(word) for word in review.words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are \"excellent\" stemmed to \"excel\" and \"amazing\" stemmed to \"amaz\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a more refined process that uses specific language and grammar rules to derive the root of a word.  \n",
    "\n",
    "This is useful for words that do not share an obvious root such as \"better\" and \"best\".\n",
    "\n",
    "- **What:** Lemmatization derives the canonical form (\"lemma\") of a word.\n",
    "- **Why:** It can be better than stemming.\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume every word is a noun.\n",
    "# notice Was is Wa\n",
    "# you can lemmitize bc this is a textblob\n",
    "print([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are \"filled\" lemmatized to \"fill\" and \"was\" lemmatized to \"wa\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume every word is a verb.\n",
    "print([word.lemmatize(pos='v') for word in review.words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples you can see are \"was\" lemmatized to \"be\" and \"arrived\" lemmatized to \"arrive\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More Lemmatization and Stemming Examples**\n",
    "\n",
    "|Word|Lemmatization|Stemming|\n",
    "|-----|-------------|---------|\n",
    "|shouted|shout|shout|\n",
    "|best | good|best|\n",
    "|better | good|better|\n",
    "|good | good|good|\n",
    "|hidden | hide|hidden|\n",
    "|computing |compute| comput|\n",
    "|computed |compute| comput|\n",
    "|wipes |wipe| wip|\n",
    "|wiped |wipe| wip|\n",
    "|wiping |wipe| wip|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns a list of lemmas.\n",
    "\n",
    "def split_into_lemmas(text):\n",
    "    text = str(text).lower()\n",
    "    words = TextBlob(text).words\n",
    "    return (word.lemmatize(pos='v') for word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use split_into_lemmas as the feature extraction function (Warning: SLOW!).\n",
    "vect = CountVectorizer(analyzer=split_into_lemmas, decode_error='replace')\n",
    "tokenize_test(vect)\n",
    "\n",
    "#use help() to understand the analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last 50 features\n",
    "vect.get_feature_names()[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that you should constantly be thinking about the result of each preprocessing step instead of blindly trying them without thinking. Does each type of preprocessing \"makes sense\" with the input data you are using? Is it likely to keep intact the signal and remove noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tfidf'></a>\n",
    "## Term Frequencyâ€“Inverse Document Frequency (TFâ€“IDF)\n",
    "     \n",
    "- **What:** Term frequencyâ€“inverse document frequency (TFâ€“IDF) computes the \"relative frequency\" with which a word appears in a document, compared to its frequency across all documents.\n",
    "- **Why:** It's more useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents).\n",
    "- **Notes:** It's used for search-engine scoring, text summarization, and document clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example documents\n",
    "simple_train = ['call you tonight', 'call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term frequency\n",
    "# for each term, how many tiems does it occur in each document\n",
    "#just the CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(simple_train)\n",
    "tf = pd.DataFrame(vect.transform(simple_train).todense(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document frequency\n",
    "# how many times does each term occur across the whole corpus (or all docs)\n",
    "#binary just gives 1, 0\n",
    "vect = CountVectorizer(binary=True)\n",
    "vect.fit(simple_train)\n",
    "df = pd.DataFrame(vect.transform(simple_train).todense().sum(axis=0), columns=vect.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term frequencyâ€“inverse document frequency (simple version)\n",
    "# measures how important is this word compared to how frequent it is normally\n",
    "\n",
    "pd.DataFrame(tf.values/df.values, columns=vect.get_feature_names())\n",
    "\n",
    "\n",
    "\n",
    "# call is not so useful to make distinctions, so gets a lower TI-IDF score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the TFâ€“IDF value, the more \"important\" the word is to that specific document. Here, \"cab\" is the most important and unique word in document 1, while \"please\" is the most important and unique word in document 2. TFâ€“IDF is often used for training as a replacement for word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer -- uses a slightly different implementation of the same basic idea\n",
    "\n",
    "vect = TfidfVectorizer()\n",
    "vect.fit(simple_train)\n",
    "pd.DataFrame(vect.transform(simple_train).todense(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Details of the sklearn implementation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "tl;dr the idf term has a log and some smoothing, and each row is normalized to have unit length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='yelp_tfidf'></a>\n",
    "## Using TFâ€“IDF to Summarize a Yelp Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a document-term matrix using TFâ€“IDF.\n",
    "vect = TfidfVectorizer(stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit transform Yelp data.\n",
    "# create summary of our review\n",
    "\n",
    "\n",
    "vect.fit(yelp.loc[:, 'text'])\n",
    "dtm = vect.transform(yelp.loc[:, 'text'])\n",
    "features = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    \n",
    "    # Choose a random review that is at least 300 characters.\n",
    "    review_length = 0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = yelp.text[review_id]\n",
    "        #review_text = unicode(yelp.text[review_id], 'utf-8')\n",
    "        review_length = len(review_text)\n",
    "    \n",
    "    # Create a dictionary of words and their TFâ€“IDF scores.\n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "    \n",
    "    # Print words with the top five TFâ€“IDF scores.\n",
    "    print('TOP SCORING WORDS:')\n",
    "    top_scores = sorted(list(word_scores.items()), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print(word)\n",
    "    \n",
    "    # Print five random words.\n",
    "    print(('\\n' + 'RANDOM WORDS:'))\n",
    "    random_words = np.random.choice(list(word_scores.keys()), size=5, replace=False)\n",
    "    for word in random_words:\n",
    "        print(word)\n",
    "    \n",
    "    # Print the review.\n",
    "    print(('\\n' + review_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sentiment'></a>\n",
    "## Sentiment Analysis\n",
    "\n",
    "Understanding how positive or negative a review is. There are many ways in practice to compute a sentiment value. For example:\n",
    "\n",
    "- Have a list of \"positive\" words and a list of \"negative\" words and count how many occur in a document. \n",
    "- Train a classifier given many examples of \"positive\" documents and \"negative\" documents. \n",
    "\n",
    "For the most accurate sentiment analysis, you will want to train a custom sentiment model based on documents that are particular to your application. Generic models (such as the one we are about to use!) often do not work as well as hoped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print first review\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity ranges from -1 (most negative) to 1 (most positive).\n",
    "\n",
    "review.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that accepts text and returns the polarity.\n",
    "\n",
    "def detect_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame column for sentiment (Warning: SLOW!).\n",
    "yelp.loc[:, 'sentiment'] = yelp.loc[:, 'text'].apply(detect_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of sentiment grouped by stars\n",
    "# you can see that there is a shift in sentiment\n",
    "yelp.boxplot(column='sentiment', by='stars');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reviews with most positive sentiment\n",
    "yelp.loc[yelp.loc[:, 'sentiment'] == 1, 'text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews with most negative sentiment\n",
    "yelp.loc[yelp.loc[:, 'sentiment'] == -1, 'text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sentiment in a 5-star review\n",
    "yelp.loc[(yelp.loc[:, 'stars'] == 5) & (yelp.loc[:, 'sentiment'] < -.3), 'text'].head().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive sentiment in a 1-star review\n",
    "# notice that sarcasm is hard for the algo to handle\n",
    "\n",
    "yelp.loc[(yelp.loc[:, 'stars'] == 1) & (yelp.loc[:, 'sentiment'] > .7), 'text'].head().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='add_feat'></a>\n",
    "## Bonus: Adding Features to a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will add additional features to our `CountVectorizer()`-generated feature set to hopefully improve our model.\n",
    "\n",
    "To make the best models, you will want to supplement the auto-generated features with new features you think might be important. After all, `CountVectorizer()` typically lowercases text and removes all associations between words. Or, you may have metadata to add in addition to just the text.\n",
    "\n",
    "> Remember: Although you may have hundreds of thousands of features, each data point is extremely sparse. So, if you add in a new feature, e.g., one that detects if the text is all capital letters, this new feature can still have a huge effect on the model outcome!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame that only contains the 5-star and 1-star reviews.\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['text', 'sentiment', 'cool', 'useful', 'funny']\n",
    "X = yelp_best_worst.loc[:, feature_cols]\n",
    "y = yelp_best_worst.loc[:, 'stars']\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064, 16825)\n",
      "(1022, 16825)\n"
     ]
    }
   ],
   "source": [
    "# Use CountVectorizer with text column only.\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train.text)\n",
    "X_test_dtm = vect.transform(X_test.text)\n",
    "print((X_train_dtm.shape))\n",
    "print((X_test_dtm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of other four feature columns\n",
    "X_train.drop('text', axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cast other feature columns to float and convert to a sparse matrix.\n",
    "extra = sp.sparse.csr_matrix(X_train.drop('text', axis=1).astype(float))\n",
    "extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16829)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine sparse matrices.\n",
    "X_train_dtm_extra = sp.sparse.hstack((X_train_dtm, extra))\n",
    "X_train_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1022, 16829)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Repeat for testing set.\n",
    "extra = sp.sparse.csr_matrix(X_test.drop('text', axis=1).astype(float))\n",
    "X_test_dtm_extra = sp.sparse.hstack((X_test_dtm, extra))\n",
    "X_test_dtm_extra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917808219178\n"
     ]
    }
   ],
   "source": [
    "# Use logistic regression with text column only.\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-08b1e3f905a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use logistic regression with all features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_dtm_extra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0my_pred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_dtm_extra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m-> 1216\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 431\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    304\u001b[0m                           % spmatrix.format)\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# Use logistic regression with all features.\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "logreg.fit(X_train_dtm_extra, y_train)\n",
    "y_pred_class = logreg.predict(X_test_dtm_extra)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='more_textblob'></a>\n",
    "## Bonus: Fun TextBlob Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spelling correction\n",
    "TextBlob('15 minuets late').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spellcheck\n",
    "Word('parot').spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definitions\n",
    "Word('bank').define('v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Language identification\n",
    "TextBlob('Hola amigos').detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bayes\"></a>\n",
    "\n",
    "## Appendix: Intro to Naive Bayes and Text Classification\n",
    "\n",
    "Naive Bayes is a very popular classifier because it has minimal storage requirements, is fast, can be tuned easily with more data, and has found very useful applications in text classificaton. For example, Paul Graham originally proposed using Naive Bayes to detect spam in his [Plan for Spam](http://www.paulgraham.com/spam.html).\n",
    "\n",
    "Earlier we experimented with text classification using a Naive Bayes model. What exactly are Naive Bayes classifiers? \n",
    "\n",
    "**What is Bayes?**  \n",
    "Bayes, or Bayes' Theorem, is a way to update a probability distribution given some new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the equation for Bayes.  \n",
    "\n",
    "$$P(A \\ | \\ B) = \\frac {P(B \\ | \\ A) \\times P(A)} {P(B)}$$\n",
    "\n",
    "- **$P(A \\ | \\ B)$** : Probability of `Event A` occurring given `Event B` has occurred.\n",
    "- **$P(B \\ | \\ A)$** : Probability of `Event B` occurring given `Event A` has occurred.\n",
    "- **$P(A)$** : Probability of `Event A` occurring.\n",
    "- **$P(B)$** : Probability of `Event B` occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Applying Naive Bayes Classification to Spam Filtering\n",
    "\n",
    "Let's pretend we have an email with three words: \"Send money now.\" We'll use Naive Bayes to classify it as **ham or spam.** (\"Ham\" just means not spam. It can include emails that look like spam but that you opt into!)\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) = \\frac {P(\\text{send money now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "By assuming that the features (the words) are conditionally independent, we can simplify the likelihood function:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {P(\\text{send} \\ | \\ spam) \\times P(\\text{money} \\ | \\ spam) \\times P(\\text{now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "Note that each conditional probability in the numerator is easily calculated directly from the training data!\n",
    "\n",
    "So, we can calculate all of the values in the numerator by examining a corpus of spam email:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {0.2 \\times 0.1 \\times 0.1 \\times 0.9} {P(\\text{send money now})} = \\frac {0.0018} {P(\\text{send money now})}$$\n",
    "\n",
    "We would repeat this process with a corpus of ham email:\n",
    "\n",
    "$$P(ham \\ | \\ \\text{send money now}) \\approx \\frac {0.05 \\times 0.01 \\times 0.1 \\times 0.1} {P(\\text{send money now})} = \\frac {0.000005} {P(\\text{send money now})}$$\n",
    "\n",
    "All we care about is whether spam or ham has the higher probability, and so we predict that the email is spam.\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The \"naive\" assumption of Naive Bayes (that the features are conditionally independent) is critical to making these calculations simple.\n",
    "- The normalization constant (the denominator) can be ignored since it's the same for all classes.\n",
    "- The prior probability is much less relevant once you have a lot of features.\n",
    "\n",
    "### Comparing Naive Bayes With Other Models\n",
    "\n",
    "Advantages of Naive Bayes:\n",
    "\n",
    "- Model training and prediction are very fast.\n",
    "- It's somewhat interpretable.\n",
    "- No tuning is required.\n",
    "- Features don't need scaling.\n",
    "- It's insensitive to irrelevant features (with enough observations).\n",
    "- It performs better than logistic regression when the training set is very small.\n",
    "\n",
    "Disadvantages of Naive Bayes:\n",
    "\n",
    "- If \"spam\" is dependent on non-independent combinations of individual words, it may not work well.\n",
    "- Predicted probabilities are not well calibrated.\n",
    "- Correlated features can be problematic (due to the independence assumption).\n",
    "- It can't handle negative features (with Multinomial Naive Bayes).\n",
    "- It has a higher \"asymptotic error\" than logistic regression.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusion'></a>\n",
    "## Summary\n",
    "\n",
    "- NLP techniques allow us to do machine learning with text.\n",
    "- High-level NLP tasks include part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "- Common steps for preprocessing text include splitting into words (\"tokenizing\"), discarding punctuation, converting to lowercase, and stemming/lemmatizing.\n",
    "- To apply machine learning to text, we need to convert documents into numeric vectors.\n",
    "- Bag-of-words representations ignore word order, while ngram representations preserve it to some extent.\n",
    "- TF-IDF is a powerful technique for turning the words in a document into a useful feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projects\n",
    "\n",
    "Due Thurs.:\n",
    "\n",
    "- Final Project Pt 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exit Tickets\n",
    "\n",
    "```\n",
    "=========================================\n",
    "@channel\n",
    "Exit Ticket: https://goo.gl/forms/OUw4gyTiRKMOTI3t2        \n",
    "\n",
    "#feedback\n",
    "=========================================\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
